{"role":"user","content":"Is the right path to my vllm source? [1]\n\n1:\n```\n$ ls -l ../../clones/vllm\ntotal 308\n-rwxr-xr-x  1 karl3 karl3 23503 Feb 14 12:53 CMakeLists.txt\n-rw-r--r--  1 karl3 karl3  5318 Feb 14 12:53 CODE_OF_CONDUCT.md\n-rw-r--r--  1 karl3 karl3   154 Feb 14 12:53 CONTRIBUTING.md\n-rw-r--r--  1 karl3 karl3  1366 Feb 14 12:53 DCO\n-rw-r--r--  1 karl3 karl3 12198 Feb 14 12:53 Dockerfile\n-rw-r--r--  1 karl3 karl3  2395 Feb 14 12:53 Dockerfile.arm\n-rw-r--r--  1 karl3 karl3  2739 Feb 14 12:53 Dockerfile.cpu\n-rw-r--r--  1 karl3 karl3   610 Feb 14 12:53 Dockerfile.hpu\n-rw-r--r--  1 karl3 karl3  2038 Feb 14 12:53 Dockerfile.neuron\n-rw-r--r--  1 karl3 karl3   975 Feb 14 12:53 Dockerfile.openvino\n-rw-r--r--  1 karl3 karl3  1550 Feb 14 12:53 Dockerfile.ppc64le\n-rw-r--r--  1 karl3 karl3  3910 Feb 14 12:53 Dockerfile.rocm\n-rw-r--r--  1 karl3 karl3  6407 Feb 14 12:53 Dockerfile.rocm_base\n-rw-r--r--  1 karl3 karl3   798 Feb 14 12:53 Dockerfile.tpu\n-rw-r--r--  1 karl3 karl3  2649 Feb 14 12:53 Dockerfile.xpu\n-rw-r--r--  1 karl3 karl3 11357 Feb 14 12:53 LICENSE\n-rw-r--r--  1 karl3 karl3   244 Feb 14 12:53 MANIFEST.in\n-rw-r--r--  1 karl3 karl3  9580 Feb 14 12:53 README.md\n-rw-r--r--  1 karl3 karl3   719 Feb 14 12:53 SECURITY.md\ndrwxr-xr-x  8 karl3 karl3  4096 Feb 14 12:53 benchmarks\ndrwxr-xr-x  2 karl3 karl3  4096 Feb 14 12:53 cmake\n-rw-r--r--  1 karl3 karl3 26257 Feb 14 12:53 collect_env.py\ndrwxr-xr-x 12 karl3 karl3  4096 Feb 14 12:53 csrc\ndrwxr-xr-x  3 karl3 karl3  4096 Feb 14 12:53 docs\ndrwxr-xr-x  5 karl3 karl3  4096 Feb 14 12:53 examples\n-rw-r--r--  1 karl3 karl3   875 Feb 14 12:53 find_cuda_init.py\n-rwxr-xr-x  1 karl3 karl3   329 Feb 14 12:53 format.sh\n-rw-r--r--  1 karl3 karl3  2975 Feb 14 12:53 pyproject.toml\n-rw-r--r--  1 karl3 karl3   614 Feb 14 12:53 python_only_dev.py\n-rw-r--r--  1 karl3 karl3   126 Feb 14 12:53 requirements-build.txt\n-rw-r--r--  1 karl3 karl3  1675 Feb 14 12:53 requirements-common.txt\n-rw-r--r--  1 karl3 karl3   689 Feb 14 12:53 requirements-cpu.txt\n-rw-r--r--  1 karl3 karl3   436 Feb 14 12:53 requirements-cuda.txt\n-rw-r--r--  1 karl3 karl3   152 Feb 14 12:53 requirements-dev.txt\n-rw-r--r--  1 karl3 karl3   229 Feb 14 12:53 requirements-hpu.txt\n-rw-r--r--  1 karl3 karl3    31 Feb 14 12:53 requirements-lint.txt\n-rw-r--r--  1 karl3 karl3   118 Feb 14 12:53 requirements-neuron.txt\n-rw-r--r--  1 karl3 karl3   465 Feb 14 12:53 requirements-openvino.txt\n-rw-r--r--  1 karl3 karl3   249 Feb 14 12:53 requirements-rocm-build.txt\n-rw-r--r--  1 karl3 karl3   161 Feb 14 12:53 requirements-rocm.txt\n-rw-r--r--  1 karl3 karl3   965 Feb 14 12:53 requirements-test.in\n-rw-r--r--  1 karl3 karl3 12766 Feb 14 12:53 requirements-test.txt\n-rw-r--r--  1 karl3 karl3  1092 Feb 14 12:53 requirements-tpu.txt\n-rw-r--r--  1 karl3 karl3   616 Feb 14 12:53 requirements-xpu.txt\n-rwxr-xr-x  1 karl3 karl3 24969 Feb 14 12:53 setup.py\ndrwxr-xr-x 42 karl3 karl3  4096 Feb 14 12:53 tests\ndrwxr-xr-x  3 karl3 karl3  4096 Feb 14 12:53 tools\n-rw-r--r--  1 karl3 karl3   584 Feb 14 12:53 use_existing_torch.py\ndrwxr-xr-x 29 karl3 karl3  4096 Feb 14 12:53 vllm\n\n```\n","ts":1.739556527965E9}
{"role":"assistant","content":"Based on the directory listing you provided, it appears that you are in the correct path to the `vllm` source code. The directory contains various files and folders that are typically part of a project repository, including:\n\n- **Files like `CMakeLists.txt`, `Dockerfile`, `README.md`, `LICENSE`, and `setup.py`:** These are common files found in open-source projects, especially those involving C++ (CMake), Docker containers, and Python packaging.\n- **Directories like `benchmarks`, `csrc`, `docs`, `examples`, `tests`, and `vllm`:** These contain the actual source code, benchmarks, documentation, and test cases for the project.\n- **Requirements files like `requirements-*.txt`:** These list the dependencies for different configurations (e.g., CPU, GPU, ROCm, TPU).\n\nThe presence of the `vllm` directory (which is likely the main Python package) and the other supporting files confirms that this is the correct path to the `vllm` source code. If you're looking to work with or modify the `v","finish_reason":"runtime_error Resource temporarily unavailable","ts":1.739556558282E9}
{"role":"assistant","content":"llm` project, this is the right place.\n\nLet me know if you need further assistance!","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[-4.1250623762607574E-3],\"tokens\":[\"token_id:1\"]},\"token_ids\":[1]}","ts":1.739556561861E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[-3.2297612633556128E-3],\"tokens\":[\"token_id:1\"]},\"token_ids\":[1]}","ts":1.739556562355E9}
{"role":"assistant","content":"","finish_reason":"stop","ts":1.739556562861E9}
{"role":"user","content":"I'd like to verify that greedy sampling is done correctly in vllm, or show that it is possible to enable it. I thought this would be done with do_sample=False but I don't see this being used. [2]\n\n2:\n```\n$ grep -r do_sample ../../clones/vllm ../../clones/transformers\n../../clones/vllm/vllm/spec_decode/mqa_scorer.py:            # n_sampled (requests with do_sample=True) to n_prompt+n_prefills.\n../../clones/vllm/vllm/spec_decode/mqa_scorer.py:                elif seq_meta.do_sample:\n../../clones/vllm/vllm/spec_decode/mqa_scorer.py:                if seq_meta.do_sample:\n../../clones/vllm/vllm/spec_decode/spec_decode_worker.py:            if not seq_group_meta.do_sample:\n../../clones/vllm/vllm/spec_decode/spec_decode_worker.py:                if sg.do_sample\n../../clones/vllm/vllm/spec_decode/spec_decode_worker.py:                sg for sg in seq_group_metadata_list if sg.do_sample\n../../clones/vllm/vllm/spec_decode/spec_decode_worker.py:            if sg.do_sample:\n../../clones/vllm/vllm/spec_decode/batch_expansion.py:        # n_sampled (requests with do_sample=True) to n_prompt+n_prefills.\n../../clones/vllm/vllm/spec_decode/batch_expansion.py:                if contracted_seq_group_metadata_list[idx].do_sample\n../../clones/vllm/vllm/sequence.py:    do_sample: bool = True\n../../clones/vllm/vllm/sequence.py:        do_sample: True if sampling is required. Sampling is not required when\n../../clones/vllm/vllm/sequence.py:    do_sample: bool = True\n../../clones/vllm/vllm/sequence.py:        # do_sample is true, only when the token_chunk_size matches the\n../../clones/vllm/vllm/sequence.py:        return self.is_prompt and self.do_sample\n../../clones/vllm/vllm/sequence.py:        self.do_sample = sequence_group_metadata_delta.do_sample\n../../clones/vllm/vllm/core/scheduler.py:            do_sample = True\n../../clones/vllm/vllm/core/scheduler.py:                    do_sample = False\n../../clones/vllm/vllm/core/scheduler.py:                    do_sample=do_sample,\n../../clones/vllm/vllm/core/scheduler.py:                    do_sample=do_sample,\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:    def do_sample(self):\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:        do_sample = seq_group_metadata.do_sample\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:                                  if do_sample else query_len)\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:            sample_len = num_prefill_sample if do_sample else 0\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:            sample_len = len(seq_ids) * query_len if do_sample else 0\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:        if do_sample:\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:        if do_sample:\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:            if seq_group.do_sample:\n../../clones/vllm/vllm/model_executor/sampling_metadata.py:                if seq_group.do_sample:\n../../clones/vllm/vllm/model_executor/layers/sampler.py:        if not seq_group.do_sample:\n../../clones/vllm/vllm/model_executor/layers/sampler.py:            seq_group.do_sample is False.\n../../clones/vllm/vllm/model_executor/layers/sampler.py:        seq_group has do_sample=False, tuple contains ([], [])\n../../clones/vllm/vllm/model_executor/layers/sampler.py:        if not seq_group.do_sample:\n../../clones/vllm/vllm/model_executor/layers/sampler.py:            seq_group.do_sample is False.\n../../clones/vllm/vllm/model_executor/layers/sampler.py:        seq_group has do_sample=False, tuple contains ([], [])\n../../clones/vllm/vllm/model_executor/layers/sampler.py:        if not seq_group.do_sample:\n../../clones/vllm/vllm/model_executor/layers/sampler.py:        seq_group has do_sample=False, tuple contains ([], [])\n../../clones/vllm/vllm/model_executor/layers/sampler.py:        if not seq_group.do_sample:\n../../clones/vllm/vllm/model_executor/layers/sampler.py:        if seq_group.do_sample:\n../../clones/vllm/vllm/model_executor/layers/sampler.py:    if seq_group.do_sample:\n../../clones/vllm/vllm/engine/llm_engine.py:                if seq_group_meta.do_sample:\n../../clones/vllm/vllm/engine/llm_engine.py:            if seq_group_metadata.do_sample:\n../../clones/vllm/vllm/worker/model_runner.py:        elif not self.chunked_prefill_enabled or seq_group_metadata.do_sample:\n../../clones/vllm/vllm/worker/neuron_model_runner.py:                do_sample=True,\n../../clones/vllm/benchmarks/backend_request_func.py:            \"do_sample\": True,\n../../clones/vllm/benchmarks/benchmark_throughput.py:            do_sample=True,\n../../clones/vllm/tests/prompt_adapter/test_bloom.py:def do_sample(llm, pa_name: str, pa_id: int):\n../../clones/vllm/tests/prompt_adapter/test_bloom.py:    assert do_sample(llm, \"twitter_pa\", pa_id=1) == expected_output\n../../clones/vllm/tests/prompt_adapter/test_pa_lora.py:def do_sample(engine):\n../../clones/vllm/tests/prompt_adapter/test_pa_lora.py:    result = do_sample(engine)\n../../clones/vllm/tests/prompt_adapter/test_multi_adapter_inference.py:def do_sample(engine):\n../../clones/vllm/tests/prompt_adapter/test_multi_adapter_inference.py:    assert do_sample(engine) == expected_output\n../../clones/vllm/tests/spec_decode/utils.py:                do_sample=idx + chunk_size >= len(prompt),  # terminal chunk\n../../clones/vllm/tests/lora/test_minicpmv_tp.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> List[str]:\n../../clones/vllm/tests/lora/test_minicpmv_tp.py:    output1 = do_sample(llm, minicpmv_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_minicpmv_tp.py:    output2 = do_sample(llm, minicpmv_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_minicpmv_tp.py:    output_tp = do_sample(llm, minicpmv_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_minicpmv_tp.py:    output_tp = do_sample(llm, minicpmv_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_minicpmv_tp.py:    output_tp = do_sample(llm, minicpmv_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_baichuan.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> List[str]:\n../../clones/vllm/tests/lora/test_baichuan.py:    output1 = do_sample(llm, baichuan_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_baichuan.py:    output2 = do_sample(llm, baichuan_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_baichuan.py:    output_tp1 = do_sample(llm_tp1, baichuan_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_baichuan.py:    output_tp2 = do_sample(llm_tp2, baichuan_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_baichuan.py:    output_tp4 = do_sample(llm_tp4, baichuan_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_gemma.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> List[str]:\n../../clones/vllm/tests/lora/test_gemma.py:    output1 = do_sample(llm, gemma_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_gemma.py:    output2 = do_sample(llm, gemma_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_quant_model.py:def do_sample(llm: vllm.LLM,\n../../clones/vllm/tests/lora/test_quant_model.py:    output = do_sample(llm,\n../../clones/vllm/tests/lora/test_quant_model.py:    output = do_sample(llm,\n../../clones/vllm/tests/lora/test_quant_model.py:    output = do_sample(llm,\n../../clones/vllm/tests/lora/test_quant_model.py:    output = do_sample(llm,\n../../clones/vllm/tests/lora/test_quant_model.py:    output_tp1 = do_sample(llm_tp1, tinyllama_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_quant_model.py:    output_tp2 = do_sample(llm_tp2, tinyllama_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_qwen2vl.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> List[str]:\n../../clones/vllm/tests/lora/test_qwen2vl.py:    output1 = do_sample(llm, qwen2vl_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_qwen2vl.py:    output2 = do_sample(llm, qwen2vl_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_llama_tp.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> List[str]:\n../../clones/vllm/tests/lora/test_llama_tp.py:    assert do_sample(llm, sql_lora_files, lora_id=0) == EXPECTED_NO_LORA_OUTPUT\n../../clones/vllm/tests/lora/test_llama_tp.py:    assert do_sample(llm, sql_lora_files, lora_id=1) == EXPECTED_LORA_OUTPUT\n../../clones/vllm/tests/lora/test_llama_tp.py:    assert do_sample(llm, sql_lora_files, lora_id=0) == EXPECTED_NO_LORA_OUTPUT\n../../clones/vllm/tests/lora/test_llama_tp.py:    assert do_sample(llm, sql_lora_files, lora_id=2) == EXPECTED_LORA_OUTPUT\n../../clones/vllm/tests/lora/test_mixtral.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int,\n../../clones/vllm/tests/lora/test_mixtral.py:    assert do_sample(llm, mixtral_lora_files, lora_id=1,\n../../clones/vllm/tests/lora/test_mixtral.py:    assert do_sample(llm, mixtral_lora_files, lora_id=2,\n../../clones/vllm/tests/lora/test_mixtral.py:    assert do_sample(llm,\n../../clones/vllm/tests/lora/test_mixtral.py:    assert do_sample(llm,\n../../clones/vllm/tests/lora/test_phi.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> List[str]:\n../../clones/vllm/tests/lora/test_phi.py:    output1 = do_sample(llm, phi2_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_phi.py:    output2 = do_sample(llm, phi2_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_chatglm3_tp.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> List[str]:\n../../clones/vllm/tests/lora/test_chatglm3_tp.py:    output1 = do_sample(llm, chatglm3_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_chatglm3_tp.py:    output2 = do_sample(llm, chatglm3_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_chatglm3_tp.py:    output1 = do_sample(llm, chatglm3_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_chatglm3_tp.py:    output2 = do_sample(llm, chatglm3_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_chatglm3_tp.py:    output1 = do_sample(llm, chatglm3_lora_files, lora_id=1)\n../../clones/vllm/tests/lora/test_chatglm3_tp.py:    output2 = do_sample(llm, chatglm3_lora_files, lora_id=2)\n../../clones/vllm/tests/lora/test_lora_bias_e2e.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int) -> List[str]:\n../../clones/vllm/tests/lora/test_lora_bias_e2e.py:    output1 = do_sample(llm, lora_bias_files, lora_id=0)\n../../clones/vllm/tests/lora/test_lora_bias_e2e.py:    output2 = do_sample(llm, lora_bias_files, lora_id=1)\n../../clones/vllm/tests/lora/test_jamba.py:def do_sample(llm: vllm.LLM, lora_path: str, lora_id: int,\n../../clones/vllm/tests/lora/test_jamba.py:    assert do_sample(llm, jamba_lora_files, lora_id=1,\n../../clones/vllm/tests/multi_step/test_correctness_llm.py:    #      * `do_sample=False` for this prompt in this step\n../../clones/vllm/tests/multi_step/test_correctness_llm.py:    # in the model runner resulting from `do_sample=False`.\n../../clones/vllm/tests/conftest.py:                                do_sample=False,\n../../clones/vllm/tests/conftest.py:                                do_sample=False,\n../../clones/vllm/tests/conftest.py:                do_sample=False,\n../../clones/vllm/tests/conftest.py:                do_sample=False,\n../../clones/vllm/tests/conftest.py:                do_sample=False,\n../../clones/vllm/tests/samplers/test_sampler.py:def _do_sample(\n../../clones/vllm/tests/samplers/test_sampler.py:    sampler_output = _do_sample(batch_size, fake_logits, sampler,\n../../clones/vllm/tests/samplers/test_sampler.py:    sampler_output = _do_sample(batch_size, fake_logits, sampler,\n../../clones/vllm/tests/samplers/test_sampler.py:    sampler_output = _do_sample(batch_size, fake_logits, sampler,\n../../clones/vllm/tests/samplers/test_sampler.py:    first_sampler_output = _do_sample(batch_size, fake_logits, sampler,\n../../clones/vllm/tests/samplers/test_sampler.py:    second_sampler_output = _do_sample(batch_size, fake_logits, sampler,\n../../clones/vllm/tests/samplers/test_sampler.py:                                         do_sample=True)\n../../clones/vllm/tests/samplers/test_sampler.py:    sampler_output = _do_sample(batch_size, fake_logits, sampler,\n../../clones/vllm/tests/samplers/test_sampler.py:        fallback_sampler_output = _do_sample(batch_size, fake_logits, sampler,\n../../clones/vllm/tests/samplers/test_sampler.py:        sampler_output = _do_sample(batch_size, fake_logits, sampler,\n../../clones/vllm/tests/models/decoder_only/vision_language/vlm_utils/model_utils.py:    def _generate(self, max_new_tokens=None, do_sample=None, **kwargs):\n../../clones/vllm/tests/models/decoder_only/vision_language/vlm_utils/model_utils.py:                do_sample=do_sample,\n../../clones/transformers/examples/research_projects/longform-qa/eli5_app.py:            do_sample=sampling,\n../../clones/transformers/examples/research_projects/longform-qa/eli5_utils.py:    do_sample=False,\n../../clones/transformers/examples/research_projects/longform-qa/eli5_utils.py:        do_sample=do_sample,\n../../clones/transformers/examples/research_projects/longform-qa/eli5_utils.py:        num_beams=1 if do_sample else n_beams,\n../../clones/transformers/examples/research_projects/synthid_text/utils.py:            do_sample=True,\n../../clones/transformers/examples/research_projects/synthid_text/detector_training.py:            do_sample=True,\n../../clones/transformers/examples/research_projects/codeparrot/README.md:--do_sample True \\\n../../clones/transformers/examples/research_projects/codeparrot/scripts/human_eval.py:        \"do_sample\": args.do_sample,\n../../clones/transformers/examples/research_projects/codeparrot/scripts/arguments.py:    do_sample: Optional[bool] = field(\n../../clones/transformers/examples/pytorch/text-generation/run_generation.py:        do_sample=True,\n../../clones/transformers/src/transformers/generation/utils.py:        - *greedy decoding* if `num_beams=1` and `do_sample=False`\n../../clones/transformers/src/transformers/generation/utils.py:        - *multinomial sampling* if `num_beams=1` and `do_sample=True`\n../../clones/transformers/src/transformers/generation/utils.py:        - *beam-search decoding* if `num_beams>1` and `do_sample=False`\n../../clones/transformers/src/transformers/generation/utils.py:        - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n../../clones/transformers/src/transformers/generation/utils.py:        if generation_config.do_sample:\n../../clones/transformers/src/transformers/generation/utils.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/generation/utils.py:            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n../../clones/transformers/src/transformers/generation/utils.py:        do_sample = generation_config.do_sample\n../../clones/transformers/src/transformers/generation/utils.py:            if do_sample:  # sample\n../../clones/transformers/src/transformers/generation/utils.py:        do_sample = generation_config.do_sample\n../../clones/transformers/src/transformers/generation/utils.py:            if do_sample:\n../../clones/transformers/src/transformers/generation/utils.py:        do_sample = generation_config.do_sample\n../../clones/transformers/src/transformers/generation/utils.py:            if do_sample:\n../../clones/transformers/src/transformers/generation/utils.py:        **sample** (depending on `do_sample`), assisted by candidate sequences. Assisted generation is an example of a\n../../clones/transformers/src/transformers/generation/utils.py:        do_sample = generation_config.do_sample\n../../clones/transformers/src/transformers/generation/utils.py:            # Case 1: `do_sample=True` and we have logits for the candidates (originally from speculative decoding)\n../../clones/transformers/src/transformers/generation/utils.py:            if do_sample and candidate_logits is not None:\n../../clones/transformers/src/transformers/generation/utils.py:                if do_sample:\n../../clones/transformers/src/transformers/generation/flax_utils.py:              `do_sample=False`\n../../clones/transformers/src/transformers/generation/flax_utils.py:              `do_sample=True`\n../../clones/transformers/src/transformers/generation/flax_utils.py:              `do_sample=False`\n../../clones/transformers/src/transformers/generation/flax_utils.py:        if not generation_config.do_sample and generation_config.num_beams == 1:\n../../clones/transformers/src/transformers/generation/flax_utils.py:        elif generation_config.do_sample and generation_config.num_beams == 1:\n../../clones/transformers/src/transformers/generation/flax_utils.py:        elif not generation_config.do_sample and generation_config.num_beams > 1:\n../../clones/transformers/src/transformers/generation/watermarking.py:    >>> out_watermarked = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=False, max_length=20)\n../../clones/transformers/src/transformers/generation/watermarking.py:    >>> out = model.generate(**inputs, do_sample=False, max_length=20)\n../../clones/transformers/src/transformers/generation/configuration_utils.py:        - *greedy decoding* if `num_beams=1` and `do_sample=False`\n../../clones/transformers/src/transformers/generation/configuration_utils.py:        - *multinomial sampling* if `num_beams=1` and `do_sample=True`\n../../clones/transformers/src/transformers/generation/configuration_utils.py:        - *beam-search decoding* if `num_beams>1` and `do_sample=False`\n../../clones/transformers/src/transformers/generation/configuration_utils.py:        - *beam-search multinomial sampling* if `num_beams>1` and `do_sample=True`\n../../clones/transformers/src/transformers/generation/configuration_utils.py:        do_sample (`bool`, *optional*, defaults to `False`):\n../../clones/transformers/src/transformers/generation/configuration_utils.py:        self.do_sample = kwargs.pop(\"do_sample\", False)\n../../clones/transformers/src/transformers/generation/configuration_utils.py:            if self.do_sample is False:\n../../clones/transformers/src/transformers/generation/configuration_utils.py:            elif self.do_sample is True:\n../../clones/transformers/src/transformers/generation/configuration_utils.py:        if self.do_sample is False:\n../../clones/transformers/src/transformers/generation/configuration_utils.py:                \"`do_sample` is set to `False`. However, `{flag_name}` is set to `{flag_value}` -- this flag is only \"\n../../clones/transformers/src/transformers/generation/configuration_utils.py:                \"used in sample-based generation modes. You should set `do_sample=True` or unset `{flag_name}`.\"\n../../clones/transformers/src/transformers/generation/configuration_utils.py:                if self.do_sample is True:\n../../clones/transformers/src/transformers/generation/configuration_utils.py:                        constrained_wrong_parameter_msg.format(flag_name=\"do_sample\", flag_value=self.do_sample)\n../../clones/transformers/src/transformers/generation/configuration_utils.py:                if self.do_sample is True:\n../../clones/transformers/src/transformers/generation/configuration_utils.py:                    raise ValueError(group_error_prefix + \"`do_sample` must be set to `False`\")\n../../clones/transformers/src/transformers/generation/configuration_utils.py:                if self.do_sample is False:\n../../clones/transformers/src/transformers/generation/configuration_utils.py:        ...     \"openai-community/gpt2\", top_k=1, foo=False, do_sample=True, return_unused_kwargs=True\n../../clones/transformers/src/transformers/generation/configuration_utils.py:    ...     **tokenized_prompts, watermarking_config=watermarking_config, do_sample=True, max_new_tokens=10\n../../clones/transformers/src/transformers/generation/configuration_utils.py:    ...     input, do_sample=False, max_new_tokens=300, cache_implementation=\"static\", compile_config=compile_config\n../../clones/transformers/src/transformers/generation/logits_process.py:    Make sure that `do_sample=True` is included in the `generate` arguments otherwise the temperature value won't have\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> generate_kwargs = {\"max_new_tokens\": 10, \"do_sample\": True, \"temperature\": 1.0, \"num_return_sequences\": 2}\n../../clones/transformers/src/transformers/generation/logits_process.py:                except_msg += \" If you're looking for greedy decoding strategies, set `do_sample=False`.\"\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True, top_p=0.1)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True, top_k=2)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True, min_p=0.1)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True)\n../../clones/transformers/src/transformers/generation/logits_process.py:    ...     **inputs, do_sample=True, typical_p=0.1, return_dict_in_generate=True, output_scores=True\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True, epsilon_cutoff=0.1)\n../../clones/transformers/src/transformers/generation/logits_process.py:    Sampling as Language Model Desmoothing](https://arxiv.org/abs/2210.15191) for more information. Note: `do_sample`\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True, eta_cutoff=0.1)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> summary_ids = model.generate(inputs[\"input_ids\"], max_new_tokens=4, do_sample=False)\n../../clones/transformers/src/transformers/generation/logits_process.py:    ...     inputs[\"input_ids\"], max_new_tokens=4, do_sample=False, sequence_bias=sequence_bias\n../../clones/transformers/src/transformers/generation/logits_process.py:    ...     inputs[\"input_ids\"], max_new_tokens=4, do_sample=False, sequence_bias=sequence_bias\n../../clones/transformers/src/transformers/generation/logits_process.py:    ...     inputs[\"input_ids\"], max_new_tokens=4, num_beams=4, do_sample=False, sequence_bias=sequence_bias\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> outputs = model.generate(**inputs, do_sample=True, temperature=0.9, max_length=30, pad_token_id=50256)\n../../clones/transformers/src/transformers/generation/logits_process.py:    ...     do_sample=True,\n../../clones/transformers/src/transformers/generation/logits_process.py:    ...     do_sample=True,\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> out = model.generate(inputs[\"input_ids\"], max_length=20, do_sample=False)\n../../clones/transformers/src/transformers/generation/logits_process.py:    >>> out = model.generate(inputs[\"input_ids\"], watermarking_config=watermarking_config, max_length=20, do_sample=False)\n../../clones/transformers/src/transformers/generation/logits_process.py:    ...     **tokenized_prompts, watermarking_config=watermarking_config, do_sample=True, max_new_tokens=10\n../../clones/transformers/src/transformers/generation/tf_utils.py:          `do_sample=False`\n../../clones/transformers/src/transformers/generation/tf_utils.py:          `do_sample=True`\n../../clones/transformers/src/transformers/generation/tf_utils.py:        parameters to generate, e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/generation/tf_utils.py:                Random seed to control sampling, containing two integers, used when `do_sample` is `True`. See the\n../../clones/transformers/src/transformers/generation/tf_utils.py:            and generation_config.do_sample is False\n../../clones/transformers/src/transformers/generation/tf_utils.py:            and generation_config.do_sample is False\n../../clones/transformers/src/transformers/generation/tf_utils.py:            and generation_config.do_sample is False\n../../clones/transformers/src/transformers/generation/tf_utils.py:        is_sample_gen_mode = (generation_config.num_beams == 1) and generation_config.do_sample is True\n../../clones/transformers/src/transformers/generation/tf_utils.py:        is_beam_sample_gen_mode = (generation_config.num_beams > 1) and generation_config.do_sample is True\n../../clones/transformers/src/transformers/generation/tf_utils.py:                do_sample=True,\n../../clones/transformers/src/transformers/generation/tf_utils.py:                Random seed to control sampling, containing two integers, used when `do_sample` is `True`. See the\n../../clones/transformers/src/transformers/generation/tf_utils.py:        do_sample: bool = False,\n../../clones/transformers/src/transformers/generation/tf_utils.py:        Generates sequences for models with a language modeling head using beam search. If `do_sample` is `False`, uses\n../../clones/transformers/src/transformers/generation/tf_utils.py:            do_sample (`bool`, *optional*, defaults to `False`):\n../../clones/transformers/src/transformers/generation/tf_utils.py:            if do_sample:\n../../clones/transformers/src/transformers/generation/tf_utils.py:            if do_sample:\n../../clones/transformers/src/transformers/generation/tf_utils.py:                output_cls = TFBeamSampleEncoderDecoderOutput if do_sample else TFBeamSearchEncoderDecoderOutput\n../../clones/transformers/src/transformers/generation/tf_utils.py:                output_cls = TFBeamSampleDecoderOnlyOutput if do_sample else TFBeamSearchDecoderOnlyOutput\n../../clones/transformers/src/transformers/pipelines/text_generation.py:    >>> generator(\"I can't believe you did such a \", do_sample=False)\n../../clones/transformers/src/transformers/pipelines/text_generation.py:    >>> generator([{\"role\": \"user\", \"content\": \"What is the capital of France? Answer in one word.\"}], do_sample=False, max_new_tokens=2)\n../../clones/transformers/src/transformers/pipelines/text_to_audio.py:    ...     \"do_sample\": True,\n../../clones/transformers/src/transformers/configuration_utils.py:            \"do_sample\": False,\n../../clones/transformers/src/transformers/commands/chat.py:    \"do_sample\",\n../../clones/transformers/src/transformers/commands/chat.py:        do_sample (`bool`, *optional*, defaults to `True`):\n../../clones/transformers/src/transformers/commands/chat.py:    do_sample: bool = field(default=True, metadata={\"help\": \"Whether to sample outputs during generation.\"})\n../../clones/transformers/src/transformers/commands/chat.py:                    \"do_sample\": current_args.do_sample,\n../../clones/transformers/src/transformers/models/mllama/convert_mllama_weights_to_hf.py:            do_sample=True,\n../../clones/transformers/src/transformers/models/mllama/modeling_mllama.py:        >>> generate_ids = model.generate(inputs.input_ids, max_length=40, do_sample=True, temperature=0.6)\n../../clones/transformers/src/transformers/models/moshi/convert_moshi_transformers.py:        do_sample=True,\n../../clones/transformers/src/transformers/models/moshi/convert_moshi_transformers.py:        do_sample=True,\n../../clones/transformers/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:        For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)` will successively perform\n../../clones/transformers/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:        if (temperature is None or temperature == 1.0) or not kwargs_speech.get(\"do_sample\", False):\n../../clones/transformers/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:        For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)` will successively perform\n../../clones/transformers/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:        if (temperature is None or temperature == 1.0) or not kwargs_speech.get(\"do_sample\", False):\n../../clones/transformers/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:        For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)` will successively\n../../clones/transformers/src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py:        if (temperature is None or temperature == 1.0) or not kwargs_speech.get(\"do_sample\", False):\n../../clones/transformers/src/transformers/models/pop2piano/modeling_pop2piano.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`. For an overview of generation\n../../clones/transformers/src/transformers/models/musicgen/convert_musicgen_transformers.py:    model.generation_config.do_sample = True\n../../clones/transformers/src/transformers/models/musicgen/modeling_musicgen.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/musicgen/modeling_musicgen.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/instructblipvideo/convert_instructblipvideo_original_to_pytorch.py:        do_sample=False,\n../../clones/transformers/src/transformers/models/instructblipvideo/modeling_instructblipvideo.py:        ...     do_sample=False,\n../../clones/transformers/src/transformers/models/instructblipvideo/modular_instructblipvideo.py:        ...     do_sample=False,\n../../clones/transformers/src/transformers/models/bark/generation_configuration_bark.py:        do_sample=False,\n../../clones/transformers/src/transformers/models/bark/generation_configuration_bark.py:            do_sample (`bool`, *optional*, defaults to `False`):\n../../clones/transformers/src/transformers/models/bark/generation_configuration_bark.py:            do_sample=do_sample,\n../../clones/transformers/src/transformers/models/bark/generation_configuration_bark.py:        do_sample=False,\n../../clones/transformers/src/transformers/models/bark/generation_configuration_bark.py:            do_sample (`bool`, *optional*, defaults to `False`):\n../../clones/transformers/src/transformers/models/bark/generation_configuration_bark.py:            do_sample=do_sample,\n../../clones/transformers/src/transformers/models/got_ocr2/convert_got_ocr2_weights_to_hf.py:    generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n../../clones/transformers/src/transformers/models/got_ocr2/modular_got_ocr2.py:        ...     do_sample=False,\n../../clones/transformers/src/transformers/models/got_ocr2/modeling_got_ocr2.py:        ...     do_sample=False,\n../../clones/transformers/src/transformers/models/emu3/convert_emu3_weights_to_hf.py:            do_sample=True,\n../../clones/transformers/src/transformers/models/emu3/convert_emu3_weights_to_hf.py:        out = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/src/transformers/models/emu3/modeling_emu3.py:        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/src/transformers/models/emu3/modeling_emu3.py:        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/src/transformers/models/emu3/modular_emu3.py:        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/src/transformers/models/emu3/modular_emu3.py:        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/src/transformers/models/nougat/convert_nougat_to_hf.py:        do_sample=False,\n../../clones/transformers/src/transformers/models/whisper/generation_whisper.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/whisper/generation_whisper.py:                The temperature to be used for generation. Passing a single `float` value and `do_sample=True` activates\n../../clones/transformers/src/transformers/models/whisper/generation_whisper.py:            generation_config.do_sample = temperature is not None and temperature > 0.0\n../../clones/transformers/src/transformers/models/whisper/generation_whisper.py:            generation_config.temperature = temperature if generation_config.do_sample else 1.0\n../../clones/transformers/src/transformers/models/whisper/generation_whisper.py:            if generation_config.do_sample:\n../../clones/transformers/src/transformers/models/whisper/generation_whisper.py:            for key in [\"do_sample\", \"temperature\", \"num_beams\"]:\n../../clones/transformers/src/transformers/models/whisper/modeling_tf_whisper.py:        parameters to generate, e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/whisper/modeling_tf_whisper.py:                Random seed to control sampling, containing two integers, used when `do_sample` is `True`. See the\n../../clones/transformers/src/transformers/models/imagegpt/modeling_imagegpt.py:        ...     input_ids=context, max_length=model.config.n_positions + 1, temperature=1.0, do_sample=True, top_k=40\n../../clones/transformers/src/transformers/models/llava_onevision/modeling_llava_onevision.py:        >>> output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py:            do_sample=True,\n../../clones/transformers/src/transformers/models/blip_2/convert_blip_2_original_to_pytorch.py:            do_sample=True,\n../../clones/transformers/src/transformers/models/instructblip/convert_instructblip_original_to_pytorch.py:        do_sample=False,\n../../clones/transformers/src/transformers/models/instructblip/modeling_instructblip.py:        ...     do_sample=False,\n../../clones/transformers/src/transformers/models/musicgen_melody/modeling_musicgen_melody.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/musicgen_melody/modeling_musicgen_melody.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/musicgen_melody/convert_musicgen_melody_transformers.py:    model.generation_config.do_sample = True\n../../clones/transformers/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:        parameters to generate(), e.g. `.generate(inputs, num_beams=4, do_sample=True)`.\n../../clones/transformers/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:        For example, calling `.generate(input_ids, num_beams=4, speech_do_sample=True)` will successively perform\n../../clones/transformers/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:        For example, calling `.generate(input_features, num_beams=4, speech_do_sample=True)` will successively perform\n../../clones/transformers/src/transformers/models/seamless_m4t/modeling_seamless_m4t.py:        For example, calling `.generate(input_ids=input_ids, num_beams=4, speech_do_sample=True)` will successively\n../../clones/transformers/src/transformers/models/chameleon/modeling_chameleon.py:        >>> generated_ids = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py:    out = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py:    out = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n../../clones/transformers/docs/source/ja/pipeline_tutorial.md:output = pipe(\"これは素晴らしい例です！\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/ja/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/ja/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ja/tasks/prompting.md:ここでは、出力生成時にモデルがもう少し柔軟になるように `do_sample=True` と `top_k=10` を追加しました。\n../../clones/transformers/docs/source/ja/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ja/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ja/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ja/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ja/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ja/tasks/summarization.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/ja/tasks/summarization.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/ja/tasks/translation.md:>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n../../clones/transformers/docs/source/ja/tasks/translation.md:>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n../../clones/transformers/docs/source/ja/tasks/language_modeling.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n../../clones/transformers/docs/source/ja/tasks/language_modeling.md:>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n../../clones/transformers/docs/source/ja/generation_strategies.md:>>> my_model.generate(**inputs, num_beams=4, do_sample=True)  # doctest: +SKIP\n../../clones/transformers/docs/source/ja/generation_strategies.md:- `do_sample`: このパラメータを`True`に設定すると、多項分布サンプリング、ビームサーチ多項分布サンプリング、Top-Kサンプリング、Top-pサンプリングなどのデコーディング戦略が有効になります。これらの戦略は、各戦略固有の調整を含む単語彙全体の確率分布から次のトークンを選択します。\n../../clones/transformers/docs/source/ja/generation_strategies.md:...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n../../clones/transformers/docs/source/ja/generation_strategies.md:[`generate`] はデフォルトで貪欲探索デコーディングを使用するため、有効にするためにパラメータを渡す必要はありません。これは、パラメータ `num_beams` が 1 に設定され、`do_sample=False` であることを意味します。\n../../clones/transformers/docs/source/ja/generation_strategies.md:多項分布サンプリングを有効にするには、`do_sample=True` および `num_beams=1` を設定します。\n../../clones/transformers/docs/source/ja/generation_strategies.md:>>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)\n../../clones/transformers/docs/source/ja/generation_strategies.md:その名前からもわかるように、このデコーディング戦略はビームサーチと多項サンプリングを組み合わせています。このデコーディング戦略を使用するには、`num_beams` を1より大きな値に設定し、`do_sample=True` を設定する必要があります。\n../../clones/transformers/docs/source/ja/generation_strategies.md:>>> outputs = model.generate(**inputs, num_beams=5, do_sample=True)\n../../clones/transformers/docs/source/ja/generation_strategies.md:>>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n../../clones/transformers/docs/source/ja/llm_tutorial.md:デフォルトでは、 [`~generation.GenerationConfig`] ファイルで指定されていない限り、`generate` は各イテレーションで最も可能性の高いトークンを選択します（貪欲デコーディング）。タスクに応じて、これは望ましくないことがあります。チャットボットやエッセイのような創造的なタスクでは、サンプリングが有益です。一方、音声の転写や翻訳のような入力に基づくタスクでは、貪欲デコーディングが有益です。`do_sample=True` でサンプリングを有効にできます。このトピックについての詳細は、この[ブログポスト](https://huggingface.co/blog/how-to-generate)で学ぶことができます。\n../../clones/transformers/docs/source/ja/llm_tutorial.md:>>> generated_ids = model.generate(**model_inputs, do_sample=True)\n../../clones/transformers/docs/source/ar/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/ar/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/ar/tasks/summarization.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/ar/tasks/summarization.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/ar/tasks/translation.md:>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n../../clones/transformers/docs/source/ar/tasks/translation.md:>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n../../clones/transformers/docs/source/ar/tasks/language_modeling.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n../../clones/transformers/docs/source/ar/tasks/language_modeling.md:>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n../../clones/transformers/docs/source/ar/llm_tutorial.md:بشكل افتراضي، وما لم يتم تحديده في [`~generation.GenerationConfig`] الملف، `generate` يحدد الكلمة الأكثر احتمالًا فى كل خطوة من خطوات عملية التوليد (وهذا يُعرف بالتشفير الجشع). اعتمادًا على مهمتك، قد يكون هذا غير مرغوب فيه؛ تستفيد المهام الإبداعية مثل برامج الدردشة أو كتابة مقال ستفيد من أسلوب العينة العشوائية في اختيار الكلمات، تمن ناحية أخرى، فإن المهام التي تعتمد على مدخلات محددة  مثل تحويل الصوت إلى نص أو الترجم من فك التشفير الجشع. قم بتفعيل أسلوب العينات العشوائية باستخدام `do_sample=True`، ويمكنك معرفة المزيد حول هذا الموضوع في [تدوينة المدونة](https://huggingface.co/blog/how-to-generate).\n../../clones/transformers/docs/source/ar/llm_tutorial.md:>>> generated_ids = model.generate(**model_inputs, do_sample=True)\n../../clones/transformers/docs/source/ar/llm_tutorial.md:>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n../../clones/transformers/docs/source/ar/chat_templating.md:    do_sample=True,\n../../clones/transformers/docs/source/fr/tutoriel_pipeline.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/fr/tutoriel_pipeline.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/es/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/es/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/ko/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ko/tasks/prompting.md:여기서는 모델이 출력을 생성할 때 조금 더 유연해질 수 있도록 `do_sample=True`와 `top_k=10`을 추가했습니다.\n../../clones/transformers/docs/source/ko/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ko/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ko/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ko/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ko/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/ko/tasks/summarization.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/ko/tasks/summarization.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/ko/tasks/translation.md:>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n../../clones/transformers/docs/source/ko/tasks/translation.md:>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n../../clones/transformers/docs/source/ko/tasks/language_modeling.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n../../clones/transformers/docs/source/ko/tasks/language_modeling.md:>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n../../clones/transformers/docs/source/ko/generation_strategies.md:>>> my_model.generate(**inputs, num_beams=4, do_sample=True)  # doctest: +SKIP\n../../clones/transformers/docs/source/ko/generation_strategies.md:- `do_sample`: 이 매개변수를 `True`로 설정하면, 다항 샘플링, 빔 탐색 다항 샘플링, Top-K 샘플링 및 Top-p 샘플링과 같은 디코딩 전략을 활성화합니다. 이러한 전략들은 전체 어휘에 대한 확률 분포에서 다음 토큰을 선택하며, 전략별로 특정 조정이 적용됩니다.\n../../clones/transformers/docs/source/ko/generation_strategies.md:...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n../../clones/transformers/docs/source/ko/generation_strategies.md:[`generate`]는 기본적으로 탐욕 탐색 디코딩을 사용하므로 이를 활성화하기 위해 별도의 매개변수를 지정할 필요가 없습니다. 이는 `num_beams`가 1로 설정되고 `do_sample=False`로 되어 있다는 의미입니다.\"\n../../clones/transformers/docs/source/ko/generation_strategies.md:다항 샘플링을 활성화하려면 `do_sample=True` 및 `num_beams=1`을 설정하세요.\n../../clones/transformers/docs/source/ko/generation_strategies.md:>>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)\n../../clones/transformers/docs/source/ko/generation_strategies.md:이 디코딩 전략은 이름에서 알 수 있듯이 빔 탐색과 다항 샘플링을 결합한 것입니다. 이 디코딩 전략을 사용하기 위해서는 `num_beams`를 1보다 큰 값으로 설정하고, `do_sample=True`로 설정해야 합니다.\n../../clones/transformers/docs/source/ko/generation_strategies.md:>>> outputs = model.generate(**inputs, num_beams=5, do_sample=True)\n../../clones/transformers/docs/source/ko/generation_strategies.md:추론 디코딩(보조 디코딩(assisted decoding)으로도 알려짐)은 동일한 토크나이저를 사용하는 훨씬 작은 보조 모델을 활용하여 몇 가지 후보 토큰을 생성하는 상위 모델의 디코딩 전략을 수정한 것입니다. 주 모델은 단일 전방 통과로 후보 토큰을 검증함으로써 디코딩 과정을 가속화합니다. `do_sample=True`일 경우, [추론 디코딩 논문](https://arxiv.org/pdf/2211.17192.pdf)에 소개된 토큰 검증과 재샘플링 방식이 사용됩니다.\n../../clones/transformers/docs/source/ko/generation_strategies.md:>>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n../../clones/transformers/docs/source/ko/quantization/awq.md:output = model.generate(input_ids, do_sample=True, max_length=50, pad_token_id=50256)\n../../clones/transformers/docs/source/ko/llm_tutorial.md:기본적으로 [`~generation.GenerationConfig`] 파일에서 별도로 지정하지 않으면, `generate`는 각 반복에서 가장 확률이 높은 토큰을 선택합니다(그리디 디코딩). 하려는 작업에 따라 이 방법은 바람직하지 않을 수 있습니다. 예를 들어, 챗봇이나 에세이 작성과 같은 창의적인 작업은 샘플링이 적합할 수 있습니다. 반면, 오디오를 텍스트로 변환하거나 번역과 같은 입력 기반 작업은 그리디 디코딩이 더 적합할 수 있습니다. `do_sample=True`로 샘플링을 활성화할 수 있으며, 이 주제에 대한 자세한 내용은 이 [블로그 포스트](https://huggingface.co/blog/how-to-generate)에서 볼 수 있습니다.\n../../clones/transformers/docs/source/ko/llm_tutorial.md:>>> generated_ids = model.generate(**model_inputs, do_sample=True)\n../../clones/transformers/docs/source/ko/llm_optims.md:추정 샘플링 디코딩(speculative sampling decoding)을 위해, 보조 모델 외에도 [`~GenerationMixin.generate`] 메서드에 `do_sample` 및 `temperature` 매개변수를 추가하십시오.\n../../clones/transformers/docs/source/ko/llm_optims.md:outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.7)\n../../clones/transformers/docs/source/ko/llm_optims.md:샘플링과 함께 프롬프트 조회 디코딩을 사용하려면, [`~GenerationMixin.generate`] 메서드에 `do_sample` 및 `temperature` 매개변수를 추가하십시오.\n../../clones/transformers/docs/source/ko/llm_optims.md:outputs = model.generate(**inputs, prompt_lookup_num_tokens=3, do_sample=True, temperature=0.7)\n../../clones/transformers/docs/source/ko/model_doc/mistral.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/ko/model_doc/mistral.md:>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/ko/model_doc/mistral.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/ko/model_doc/mistral.md:>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/ko/model_doc/gpt_neox_japanese.md:...     do_sample=True,\n../../clones/transformers/docs/source/ko/model_doc/cohere.md:    do_sample=True, \n../../clones/transformers/docs/source/ko/model_doc/cohere.md:    do_sample=True, \n../../clones/transformers/docs/source/ko/model_doc/cohere.md:    do_sample=True, \n../../clones/transformers/docs/source/hi/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/hi/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/de/llm_tutorial.md:Standardmäßig und sofern nicht in der Datei [`~generation.GenerationConfig`] angegeben, wählt `generate` bei jeder Iteration das wahrscheinlichste Token aus (gierige Dekodierung). Je nach Aufgabe kann dies unerwünscht sein; kreative Aufgaben wie Chatbots oder das Schreiben eines Aufsatzes profitieren vom Sampling. Andererseits profitieren Aufgaben, bei denen es auf die Eingabe ankommt, wie z.B. Audiotranskription oder Übersetzung, von der gierigen Dekodierung. Aktivieren Sie das Sampling mit `do_sample=True`. Mehr zu diesem Thema erfahren Sie in diesem [Blogbeitrag](https://huggingface.co/blog/how-to-generate).\n../../clones/transformers/docs/source/de/llm_tutorial.md:>>> generated_ids = model.generate(**model_inputs, do_sample=True)\n../../clones/transformers/docs/source/zh/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/zh/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/zh/llm_tutorial.md:默认情况下，除非在[`~generation.GenerationConfig`]文件中指定，否则`generate`会在每个迭代中选择最可能的token（贪婪解码）。对于您的任务，这可能是不理想的；像聊天机器人或写作文章这样的创造性任务受益于采样。另一方面，像音频转录或翻译这样的基于输入的任务受益于贪婪解码。通过将`do_sample=True`启用采样，您可以在这篇[博客文章](https://huggingface.co/blog/how-to-generate)中了解更多关于这个话题的信息。\n../../clones/transformers/docs/source/zh/llm_tutorial.md:>>> generated_ids = model.generate(**model_inputs, do_sample=True)\n../../clones/transformers/docs/source/zh/llm_tutorial.md:>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n../../clones/transformers/docs/source/en/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/en/pipeline_tutorial.md:output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n../../clones/transformers/docs/source/en/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/tasks/prompting.md:Here we've added a `do_sample=True` and `top_k=10` to allow the model to be a bit more flexible when generating output.\n../../clones/transformers/docs/source/en/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/tasks/prompting.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/tasks/video_text_to_text.md:output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/en/tasks/image_text_to_text.md:        \"do_sample\": False\n../../clones/transformers/docs/source/en/tasks/summarization.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/en/tasks/summarization.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/docs/source/en/tasks/translation.md:>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n../../clones/transformers/docs/source/en/tasks/translation.md:>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n../../clones/transformers/docs/source/en/tasks/language_modeling.md:>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n../../clones/transformers/docs/source/en/tasks/language_modeling.md:>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> my_model.generate(**inputs, num_beams=4, do_sample=True)  # doctest: +SKIP\n../../clones/transformers/docs/source/en/generation_strategies.md:- `do_sample`: if set to `True`, this parameter enables decoding strategies such as multinomial sampling, beam-search\n../../clones/transformers/docs/source/en/generation_strategies.md:...     max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> out = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=False, max_length=20)\n../../clones/transformers/docs/source/en/generation_strategies.md:[`generate`] uses greedy search decoding by default so you don't have to pass any parameters to enable it. This means the parameters `num_beams` is set to 1 and `do_sample=False`.\n../../clones/transformers/docs/source/en/generation_strategies.md:To enable multinomial sampling set `do_sample=True` and `num_beams=1`.\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)\n../../clones/transformers/docs/source/en/generation_strategies.md:the `num_beams` greater than 1, and set `do_sample=True` to use this decoding strategy.\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> outputs = model.generate(**inputs, num_beams=5, do_sample=True)\n../../clones/transformers/docs/source/en/generation_strategies.md:tokens in a single forward pass, which speeds up the decoding process. If `do_sample=True`, then the token validation with\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> pipe_output = pipe(\"Once upon a time, \", max_new_tokens=50, do_sample=False)\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> outputs = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> vanilla_output = model.generate(**inputs, do_sample=False, max_new_tokens=50)\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> dola_high_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers='high')\n../../clones/transformers/docs/source/en/generation_strategies.md:>>> dola_custom_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers=[28,30], repetition_penalty=1.2)\n../../clones/transformers/docs/source/en/kv_cache.md:>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"backend\": \"quanto\"})\n../../clones/transformers/docs/source/en/kv_cache.md:>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation=\"offloaded\")\n../../clones/transformers/docs/source/en/kv_cache.md:>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=23)\n../../clones/transformers/docs/source/en/kv_cache.md:>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"static\")\n../../clones/transformers/docs/source/en/kv_cache.md:>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"offloaded_static\")\n../../clones/transformers/docs/source/en/kv_cache.md:>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation=\"sliding_window\")\n../../clones/transformers/docs/source/en/kv_cache.md:>>> out = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)\n../../clones/transformers/docs/source/en/kv_cache.md:...     outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\n../../clones/transformers/docs/source/en/quantization/compressed_tensors.md:generated_ids = quantized_model.generate(**inputs, max_length=50, do_sample=False)\n../../clones/transformers/docs/source/en/quantization/vptq.md:out = model.generate(**input_ids, max_new_tokens=32, do_sample=False)\n../../clones/transformers/docs/source/en/quantization/awq.md:output = model.generate(input_ids, do_sample=True, max_length=50, pad_token_id=50256)\n../../clones/transformers/docs/source/en/quantization/awq.md:output = model.generate(input_ids, do_sample=True, max_length=50, pad_token_id=pad_token_id)\n../../clones/transformers/docs/source/en/llm_tutorial.md:By default, and unless specified in the [`~generation.GenerationConfig`] file, `generate` selects the most likely token at each iteration (greedy decoding). Depending on your task, this may be undesirable; creative tasks like chatbots or writing an essay benefit from sampling. On the other hand, input-grounded tasks like audio transcription or translation benefit from greedy decoding. Enable sampling with `do_sample=True`, and you can learn more about this topic in this [blog post](https://huggingface.co/blog/how-to-generate).\n../../clones/transformers/docs/source/en/llm_tutorial.md:>>> generated_ids = model.generate(**model_inputs, do_sample=True)\n../../clones/transformers/docs/source/en/llm_tutorial.md:>>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n../../clones/transformers/docs/source/en/chat_template_tools_and_documents.md:    do_sample=True,\n../../clones/transformers/docs/source/en/llm_optims.md:For speculative sampling decoding, add the `do_sample` and `temperature` parameters to the [`~GenerationMixin.generate`] method in addition to the assistant model.\n../../clones/transformers/docs/source/en/llm_optims.md:outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.7)\n../../clones/transformers/docs/source/en/llm_optims.md:For prompt lookup decoding with sampling, add the `do_sample` and `temperature` parameters to the [`~GenerationMixin.generate`] method.\n../../clones/transformers/docs/source/en/llm_optims.md:outputs = model.generate(**inputs, prompt_lookup_num_tokens=3, do_sample=True, temperature=0.7)\n../../clones/transformers/docs/source/en/model_doc/mixtral.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/mixtral.md:>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/mixtral.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/mixtral.md:>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/glm.md:>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/got_ocr2.md:...     do_sample=False,\n../../clones/transformers/docs/source/en/model_doc/got_ocr2.md:...     do_sample=False,\n../../clones/transformers/docs/source/en/model_doc/got_ocr2.md:...     do_sample=False,\n../../clones/transformers/docs/source/en/model_doc/got_ocr2.md:...     do_sample=False,\n../../clones/transformers/docs/source/en/model_doc/got_ocr2.md:...     do_sample=False,\n../../clones/transformers/docs/source/en/model_doc/got_ocr2.md:...     do_sample=False,\n../../clones/transformers/docs/source/en/model_doc/got_ocr2.md:...     do_sample=False,\n../../clones/transformers/docs/source/en/model_doc/musicgen_melody.md:MusicGen Melody is compatible with two generation modes: greedy and sampling. In practice, sampling leads to significantly better results than greedy, thus we encourage sampling mode to be used where possible. Sampling is enabled by default, and can be explicitly specified by setting `do_sample=True` in the call to [`MusicgenMelodyForConditionalGeneration.generate`], or by overriding the model's generation config (see below).\n../../clones/transformers/docs/source/en/model_doc/musicgen_melody.md:>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen_melody.md:>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen_melody.md:>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen_melody.md:>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen_melody.md:>>> audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen_melody.md:Note that any arguments passed to the generate method will **supersede** those in the generation config, so setting `do_sample=False` in the call to generate will supersede the setting of `model.generation_config.do_sample` in the generation config.\n../../clones/transformers/docs/source/en/model_doc/musicgen_melody.md:* Sampling mode tends to deliver better results than greedy - you can toggle sampling with the variable `do_sample` in the call to [`MusicgenMelodyForConditionalGeneration.generate`]\n../../clones/transformers/docs/source/en/model_doc/gpt2.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/model_doc/gpt2.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/mistral.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/mistral.md:>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/mistral.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/mistral.md:>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/stablelm.md:>>> generated_ids = model.generate(**model_inputs, max_length=32, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/stablelm.md:>>> generated_ids = model.generate(**model_inputs, max_length=32, do_sample=True)  # doctest: +SKIP\n../../clones/transformers/docs/source/en/model_doc/starcoder2.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=10, do_sample=False)\n../../clones/transformers/docs/source/en/model_doc/gpt-sw3.md:>>> generated_token_ids = model.generate(inputs=input_ids, max_new_tokens=10, do_sample=True)[0]\n../../clones/transformers/docs/source/en/model_doc/qwen2.md:>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/gpt_neo.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/model_doc/gpt_neo.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/aria.md:    do_sample=True,\n../../clones/transformers/docs/source/en/model_doc/gpt_neox.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/model_doc/gpt_bigcode.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n../../clones/transformers/docs/source/en/model_doc/seamless_m4t_v2.md:You can use different [generation strategies](../generation_strategies) for text generation, e.g `.generate(input_ids=input_ids, text_num_beams=4, text_do_sample=True)` which will perform multinomial beam-search decoding on the text model. Note that speech generation only supports greedy - by default - or multinomial sampling, which can be used with e.g. `.generate(..., speech_do_sample=True, speech_temperature=0.6)`.\n../../clones/transformers/docs/source/en/model_doc/gpt_neox_japanese.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/model_doc/cohere.md:    do_sample=True, \n../../clones/transformers/docs/source/en/model_doc/cohere.md:    do_sample=True, \n../../clones/transformers/docs/source/en/model_doc/cohere.md:    do_sample=True, \n../../clones/transformers/docs/source/en/model_doc/opt.md:>>> generated_ids = model.generate(**model_inputs, max_new_tokens=30, do_sample=False)\n../../clones/transformers/docs/source/en/model_doc/helium.md:>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/seamless_m4t.md:You can use different [generation strategies](./generation_strategies) for speech and text generation, e.g `.generate(input_ids=input_ids, text_num_beams=4, speech_do_sample=True)` which will successively perform beam-search decoding on the text model, and multinomial sampling on the speech model.\n../../clones/transformers/docs/source/en/model_doc/mllama.md:output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n../../clones/transformers/docs/source/en/model_doc/qwen2_moe.md:>>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n../../clones/transformers/docs/source/en/model_doc/phimoe.md:    \"do_sample\": False, \n../../clones/transformers/docs/source/en/model_doc/musicgen.md:and can be explicitly specified by setting `do_sample=True` in the call to [`MusicgenForConditionalGeneration.generate`],\n../../clones/transformers/docs/source/en/model_doc/musicgen.md:>>> audio_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen.md:>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen.md:>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen.md:>>> audio_values = model.generate(**inputs, do_sample=True, guidance_scale=3, max_new_tokens=256)\n../../clones/transformers/docs/source/en/model_doc/musicgen.md:`do_sample=False` in the call to generate will supersede the setting of `model.generation_config.do_sample` in the \n../../clones/transformers/docs/source/en/model_doc/musicgen.md:* Sampling mode tends to deliver better results than greedy - you can toggle sampling with the variable `do_sample` in the call to [`MusicgenForConditionalGeneration.generate`]\n../../clones/transformers/docs/source/en/model_doc/gptj.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/model_doc/gptj.md:...     do_sample=True,\n../../clones/transformers/docs/source/en/model_doc/t5.md:...     do_sample=False,  # disable sampling to test if batching affects output\n../../clones/transformers/docs/source/en/model_doc/cohere2.md:    do_sample=True,\n../../clones/transformers/benchmark/config/generation.yaml:    do_sample: false\n../../clones/transformers/benchmark/llama.py:        gen_config = GenerationConfig(do_sample=False, top_p=1, temperature=1)\n../../clones/transformers/benchmark/llama.py:            output = model.generate(**inputs, do_sample=False)\n../../clones/transformers/benchmark/llama.py:            model.generate(**inputs, do_sample=False)\n../../clones/transformers/tests/generation/test_framework_agnostic.py:        # typos are quickly detected (the correct argument is `do_sample`)\n../../clones/transformers/tests/generation/test_framework_agnostic.py:        with self.assertRaisesRegex(ValueError, \"do_samples\"):\n../../clones/transformers/tests/generation/test_framework_agnostic.py:            model.generate(input_ids, do_samples=True)\n../../clones/transformers/tests/generation/test_framework_agnostic.py:            do_sample=True,\n../../clones/transformers/tests/generation/test_framework_agnostic.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_framework_agnostic.py:            \"do_sample\": False,\n../../clones/transformers/tests/generation/test_framework_agnostic.py:            \"do_sample\": False,\n../../clones/transformers/tests/generation/test_framework_agnostic.py:            \"do_sample\": False,\n../../clones/transformers/tests/generation/test_configuration_utils.py:            do_sample=True,\n../../clones/transformers/tests/generation/test_configuration_utils.py:        self.assertEqual(loaded_config.do_sample, True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        self.assertEqual(default_config.do_sample, False)\n../../clones/transformers/tests/generation/test_configuration_utils.py:            do_sample=True,\n../../clones/transformers/tests/generation/test_configuration_utils.py:        self.assertEqual(config.do_sample, True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        self.assertEqual(loaded_config.do_sample, True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        # parameters with `do_sample=False`). May be escalated to an error in the future.\n../../clones/transformers/tests/generation/test_configuration_utils.py:            GenerationConfig(do_sample=False, temperature=0.5)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config_bad_temperature = GenerationConfig(do_sample=False, temperature=0.5)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config_bad_temperature = GenerationConfig(do_sample=False, temperature=0.5)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config_bad_temperature = GenerationConfig(do_sample=False, temperature=0.5)\n../../clones/transformers/tests/generation/test_configuration_utils.py:            GenerationConfig(do_sample=False, num_beams=1, num_return_sequences=2)\n../../clones/transformers/tests/generation/test_configuration_utils.py:            GenerationConfig(do_sample=True, num_beams=2, constraints=[\"dummy\"])\n../../clones/transformers/tests/generation/test_configuration_utils.py:            GenerationConfig(do_sample=True, num_beams=2, force_words_ids=[[[1, 2, 3]]])\n../../clones/transformers/tests/generation/test_configuration_utils.py:        # setting the temperature alone is invalid, as we also need to set do_sample to True -> throws a warning that\n../../clones/transformers/tests/generation/test_configuration_utils.py:        config = GenerationConfig(do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        config = GenerationConfig(top_k=10, do_sample=False, penalty_alpha=0.6)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(temperature=temperature, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(top_p=top_p, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(top_k=top_k, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(min_p=min_p, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(typical_p=mass, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(epsilon_cutoff=epsilon, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(eta_cutoff=epsilon, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(no_repeat_ngram_size=ngram_size, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:        generation_config = GenerationConfig(encoder_no_repeat_ngram_size=ngram_size, do_sample=True)\n../../clones/transformers/tests/generation/test_configuration_utils.py:                do_sample=True,\n../../clones/transformers/tests/generation/test_configuration_utils.py:                do_sample=True,\n../../clones/transformers/tests/generation/test_configuration_utils.py:                do_sample=True,\n../../clones/transformers/tests/generation/test_configuration_utils.py:                do_sample=True,\n../../clones/transformers/tests/generation/test_configuration_utils.py:                do_sample=True,\n../../clones/transformers/tests/generation/test_utils.py:    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n../../clones/transformers/tests/generation/test_utils.py:        if do_sample:\n../../clones/transformers/tests/generation/test_utils.py:        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=True,\n../../clones/transformers/tests/generation/test_utils.py:        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=True,\n../../clones/transformers/tests/generation/test_utils.py:        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:                do_sample=False, max_new_tokens=self.max_new_tokens, remove_invalid_values=True\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:            logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": True,\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": True,\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:            outputs = model.generate(**inputs, do_sample=False, max_new_tokens=4, return_dict_in_generate=True)\n../../clones/transformers/tests/generation/test_utils.py:            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=3, return_dict_in_generate=True)\n../../clones/transformers/tests/generation/test_utils.py:            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=1, return_dict_in_generate=True)\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:            logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n../../clones/transformers/tests/generation/test_utils.py:                do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:                \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:                    \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:        _ = model.generate(**model_inputs, watermarking_config=watermark_config, do_sample=False, max_length=15)\n../../clones/transformers/tests/generation/test_utils.py:        output = model.generate(**model_inputs, do_sample=False, max_length=15)\n../../clones/transformers/tests/generation/test_utils.py:        output_selfhash = model.generate(**model_inputs, watermarking_config=args, do_sample=False, max_length=15)\n../../clones/transformers/tests/generation/test_utils.py:                **model_inputs, watermarking_config=watermark_config, do_sample=True, max_length=500, top_k=1000\n../../clones/transformers/tests/generation/test_utils.py:        outputs = bart_model.generate(input_ids, generation_config=GenerationConfig(do_sample=False))\n../../clones/transformers/tests/generation/test_utils.py:            generation_config=GenerationConfig(do_sample=False, decoder_start_token_id=decoder_start_token_id),\n../../clones/transformers/tests/generation/test_utils.py:            outputs = bart_model.generate(input_ids, generation_config=GenerationConfig(do_sample=False))\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=True,\n../../clones/transformers/tests/generation/test_utils.py:            \"do_sample\": True,\n../../clones/transformers/tests/generation/test_utils.py:            \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:            \"do_sample\": False,\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=True,\n../../clones/transformers/tests/generation/test_utils.py:        expected_out = target_model.generate(input_ids, do_sample=False, max_new_tokens=max_new_tokens_item)\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:            do_sample=False,\n../../clones/transformers/tests/generation/test_utils.py:        original_outputs = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n../../clones/transformers/tests/generation/test_utils.py:        outputs_assisted = model.generate(**inputs, assistant_early_exit=4, do_sample=False, max_new_tokens=20)\n../../clones/transformers/tests/generation/test_utils.py:        model.generate(input_ids, do_sample=True, max_time=MAX_TIME, max_length=MAX_LENGTH)\n../../clones/transformers/tests/generation/test_utils.py:        model.generate(input_ids, do_sample=False, max_time=MAX_TIME, max_length=MAX_LENGTH)\n../../clones/transformers/tests/generation/test_utils.py:        model.generate(input_ids, do_sample=False, num_beams=2, max_time=MAX_TIME, max_length=MAX_LENGTH)\n../../clones/transformers/tests/generation/test_utils.py:        model.generate(input_ids, do_sample=False, max_time=None, max_length=MAX_LENGTH)\n../../clones/transformers/tests/generation/test_streamers.py:        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n../../clones/transformers/tests/generation/test_streamers.py:            model.generate(input_ids, max_new_tokens=10, do_sample=False, streamer=streamer)\n../../clones/transformers/tests/generation/test_streamers.py:        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n../../clones/transformers/tests/generation/test_streamers.py:        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n../../clones/transformers/tests/generation/test_streamers.py:        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n../../clones/transformers/tests/generation/test_streamers.py:            model.generate(input_ids, max_new_tokens=10, do_sample=False, streamer=streamer)\n../../clones/transformers/tests/generation/test_streamers.py:                model.generate(input_ids, max_new_tokens=1, do_sample=False, streamer=streamer)\n../../clones/transformers/tests/generation/test_streamers.py:        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n../../clones/transformers/tests/generation/test_streamers.py:        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n../../clones/transformers/tests/generation/test_streamers.py:        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n../../clones/transformers/tests/generation/test_streamers.py:        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n../../clones/transformers/tests/generation/test_tf_utils.py:            \"do_sample\": True,\n../../clones/transformers/tests/generation/test_flax_utils.py:        config.do_sample = False\n../../clones/transformers/tests/generation/test_flax_utils.py:        config.do_sample = False\n../../clones/transformers/tests/generation/test_flax_utils.py:        config.do_sample = True\n../../clones/transformers/tests/generation/test_flax_utils.py:        config.do_sample = False\n../../clones/transformers/tests/generation/test_flax_utils.py:        config.do_sample = False\n../../clones/transformers/tests/generation/test_flax_utils.py:        config.do_sample = True\n../../clones/transformers/tests/generation/test_flax_utils.py:        config.do_sample = False\n../../clones/transformers/tests/generation/test_flax_utils.py:        config.do_sample = True\n../../clones/transformers/tests/generation/test_flax_utils.py:        # typos are quickly detected (the correct argument is `do_sample`)\n../../clones/transformers/tests/generation/test_flax_utils.py:        with self.assertRaisesRegex(ValueError, \"do_samples\"):\n../../clones/transformers/tests/generation/test_flax_utils.py:            model.generate(input_ids, do_samples=True)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        # Using `do_sample=False` to force deterministic output\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(\"This is a test\", do_sample=False)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(\"This is a test\", do_sample=True, num_return_sequences=2, return_tensors=True)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:            text_generator(test_str, do_sample=False, return_full_text=False, min_new_tokens=1)[0][\"generated_text\"],\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:                do_sample=False,\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(\"This is a test\", do_sample=False, add_special_tokens=False, padding=False)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:            do_sample=True,\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        # Using `do_sample=False` to force deterministic output\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(chat1, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator([chat1, chat2], do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        # Using `do_sample=False` to force deterministic output\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(chat1, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        # Using `do_sample=False` to force deterministic output\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(chat1, do_sample=False, max_new_tokens=10, continue_final_message=True)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        for outputs in text_generator(key_dataset, do_sample=False, max_new_tokens=10):\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        # Using `do_sample=False` to force deterministic output\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(data(), do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        # Using `do_sample=False` to force deterministic output\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(\"This is a test\", do_sample=False)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator([\"This is a test\", \"This is a second test\"], do_sample=False)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        # Using `do_sample=False` to force deterministic output\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator(chat1, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator([chat1, chat2], do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        outputs = text_generator([\"This is great !\", \"Something else\"], num_return_sequences=2, do_sample=True)\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:                [\"This is great !\", \"Something else\"], num_return_sequences=2, batch_size=2, do_sample=True\n../../clones/transformers/tests/pipelines/test_pipelines_text_generation.py:        pipe(\"This is a test\", do_sample=True, top_p=0.5)\n../../clones/transformers/tests/pipelines/test_pipelines_text2text_generation.py:        outputs = generator([\"This is great !\", \"Something else\"], num_return_sequences=2, do_sample=True)\n../../clones/transformers/tests/pipelines/test_pipelines_text2text_generation.py:            [\"This is great !\", \"Something else\"], num_return_sequences=2, batch_size=2, do_sample=True\n../../clones/transformers/tests/pipelines/test_pipelines_text2text_generation.py:        # do_sample=False necessary for reproducibility\n../../clones/transformers/tests/pipelines/test_pipelines_text2text_generation.py:        outputs = generator(\"Something there\", do_sample=False)\n../../clones/transformers/tests/pipelines/test_pipelines_text2text_generation.py:        outputs = generator(\"This is a test\", do_sample=True, num_return_sequences=2, return_tensors=True)\n../../clones/transformers/tests/pipelines/test_pipelines_text2text_generation.py:            do_sample=True,\n../../clones/transformers/tests/pipelines/test_pipelines_text2text_generation.py:        # do_sample=False necessary for reproducibility\n../../clones/transformers/tests/pipelines/test_pipelines_text2text_generation.py:        outputs = generator(\"Something there\", do_sample=False)\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            \"do_sample\": False,\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            # Using `do_sample=False` to force deterministic output\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            \"do_sample\": False,\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            \"do_sample\": True,\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            \"do_sample\": True,\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            outputs = speech_generator(\"This is a test\", forward_params={\"speaker_id\": 5, \"do_sample\": True})\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:        generate_kwargs = {\"do_sample\": True}\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            \"do_sample\": True,\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            \"do_sample\": False,\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:        generate_kwargs = {\"do_sample\": True}\n../../clones/transformers/tests/pipelines/test_pipelines_text_to_audio.py:            {\"num_return_sequences\": 2, \"do_sample\": True} if speech_generator.model.can_generate() else {}\n../../clones/transformers/tests/trainer/test_trainer_seq2seq.py:        gen_config = GenerationConfig(do_sample=False, top_p=0.9)  # bad: top_p is not compatible with do_sample=False\n../../clones/transformers/tests/test_modeling_tf_common.py:                    model.generate(do_sample=True, max_length=5)\n../../clones/transformers/tests/test_modeling_tf_common.py:                self._check_generated_ids(model.generate(input_ids, do_sample=True))\n../../clones/transformers/tests/test_modeling_tf_common.py:                self._check_generated_ids(model.generate(do_sample=True, max_length=5))\n../../clones/transformers/tests/test_modeling_tf_common.py:                model.generate(input_ids, do_sample=False, num_return_sequences=2)\n../../clones/transformers/tests/test_modeling_tf_common.py:            self._check_generated_ids(model.generate(input_ids, do_sample=True, num_return_sequences=2))\n../../clones/transformers/tests/test_modeling_tf_common.py:                input_ids, do_sample=True, bad_words_ids=bad_words_ids, num_return_sequences=2\n../../clones/transformers/tests/test_modeling_tf_common.py:                do_sample=False,\n../../clones/transformers/tests/test_modeling_tf_common.py:                do_sample=True,\n../../clones/transformers/tests/test_modeling_tf_common.py:                self._check_generated_ids(model.generate(input_ids, do_sample=True, num_beams=2))\n../../clones/transformers/tests/test_modeling_tf_common.py:                self._check_generated_ids(model.generate(do_sample=True, max_length=5, num_beams=2))\n../../clones/transformers/tests/test_modeling_tf_common.py:                model.generate(input_ids, do_sample=False, num_return_sequences=3, num_beams=2)\n../../clones/transformers/tests/test_modeling_tf_common.py:                    do_sample=True,\n../../clones/transformers/tests/test_modeling_tf_common.py:            self._check_generated_ids(model.generate(input_ids, do_sample=False, num_beams=2, num_return_sequences=2))\n../../clones/transformers/tests/test_modeling_tf_common.py:                input_ids, do_sample=False, bad_words_ids=bad_words_ids, num_beams=2, num_return_sequences=2\n../../clones/transformers/tests/test_modeling_tf_common.py:                do_sample=False,\n../../clones/transformers/tests/test_modeling_tf_common.py:                do_sample=True,\n../../clones/transformers/tests/test_modeling_tf_common.py:            config.do_sample = False\n../../clones/transformers/tests/utils/test_configuration_utils.py:    \"do_sample\": True,\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out_legacy = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=True, max_new_tokens=256, past_key_values=DynamicCache())\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10, past_key_values=DynamicCache())\n../../clones/transformers/tests/utils/test_cache_utils.py:            do_sample=False,\n../../clones/transformers/tests/utils/test_cache_utils.py:            do_sample=False,\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=3000, past_key_values=cache)\n../../clones/transformers/tests/utils/test_cache_utils.py:                input_ids, do_sample=False, max_new_tokens=100, past_key_values=cache, use_cache=True\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs_expanded, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/utils/test_cache_utils.py:        gen_out = model.generate(**inputs_expanded, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/quantization/vptq_integration/test_vptq.py:        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n../../clones/transformers/tests/quantization/vptq_integration/test_vptq.py:            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n../../clones/transformers/tests/quantization/vptq_integration/test_vptq.py:        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n../../clones/transformers/tests/quantization/quanto_integration/test_quanto.py:        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False, cache_implementation=\"quantized\")\n../../clones/transformers/tests/quantization/compressed_tensor/test_compressed_tensors.py:        generated_ids = quantized_model.generate(**inputs, max_length=50, do_sample=False)\n../../clones/transformers/tests/quantization/finegrained_fp8/test_fp8.py:        output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n../../clones/transformers/tests/quantization/finegrained_fp8/test_fp8.py:            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n../../clones/transformers/tests/quantization/finegrained_fp8/test_fp8.py:        output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n../../clones/transformers/tests/quantization/finegrained_fp8/test_fp8.py:            output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n../../clones/transformers/tests/quantization/finegrained_fp8/test_fp8.py:            output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n../../clones/transformers/tests/quantization/bitnet_integration/test_bitnet.py:        output = self.quantized_model.generate(**input_ids, max_new_tokens=11, do_sample=False)\n../../clones/transformers/tests/quantization/autoawq/test_awq.py:        output = model.generate(input_ids, do_sample=False, max_length=20, pad_token_id=pad_token_id)\n../../clones/transformers/tests/models/llava_next_video/test_modeling_llava_next_video.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=40)\n../../clones/transformers/tests/models/llava_next_video/test_modeling_llava_next_video.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n../../clones/transformers/tests/models/llava_next_video/test_modeling_llava_next_video.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=50)\n../../clones/transformers/tests/models/llava_next_video/test_modeling_llava_next_video.py:        output_batched = model.generate(**inputs_batched, do_sample=False, max_new_tokens=50)\n../../clones/transformers/tests/models/llava_next_video/test_modeling_llava_next_video.py:        output_single = model.generate(**inputs_single, do_sample=False, max_new_tokens=50)\n../../clones/transformers/tests/models/xlm/test_modeling_tf_xlm.py:        output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/xlm/test_modeling_xlm.py:        output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/mamba2/test_modeling_mamba2.py:        out = model.generate(input_ids, do_sample=False, use_cache=True, max_new_tokens=30)\n../../clones/transformers/tests/models/helium/test_modeling_helium.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/switch_transformers/test_modeling_switch_transformers.py:            input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/switch_transformers/test_modeling_switch_transformers.py:        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n../../clones/transformers/tests/models/clvp/test_modeling_clvp.py:            do_sample=False,\n../../clones/transformers/tests/models/prophetnet/test_modeling_prophetnet.py:            input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/prophetnet/test_modeling_prophetnet.py:        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n../../clones/transformers/tests/models/prophetnet/test_modeling_prophetnet.py:            input_ids[:1], num_beams=2, max_length=10, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/prophetnet/test_modeling_prophetnet.py:        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=10, do_sample=True)\n../../clones/transformers/tests/models/gemma/test_modeling_flax_gemma.py:        output = self.model.generate(**inputs, params=self.params, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_flax_gemma.py:                input_ids, attention_mask=attention_mask, params=self.params, max_new_tokens=20, do_sample=False\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:        generated_ids = model.generate(**inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False)\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:            **prompt_tokens, max_new_tokens=max_new_tokens, do_sample=False, cache_implementation=cache_implementation\n../../clones/transformers/tests/models/gemma/test_modeling_gemma.py:            **inputs, max_new_tokens=20, do_sample=False, dola_layers=\"low\", repetition_penalty=1.2\n../../clones/transformers/tests/models/blenderbot_small/test_modeling_blenderbot_small.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/diffllama/test_modeling_diffllama.py:        output_native = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/diffllama/test_modeling_diffllama.py:        output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/diffllama/test_modeling_diffllama.py:            res_eager = model_eager.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n../../clones/transformers/tests/models/diffllama/test_modeling_diffllama.py:            res_sdpa = model_sdpa.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n../../clones/transformers/tests/models/diffllama/test_modeling_diffllama.py:        generated_ids = model.generate(**inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False)\n../../clones/transformers/tests/models/diffllama/test_modeling_diffllama.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/diffllama/test_modeling_diffllama.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/glm/test_modeling_glm.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/glm/test_modeling_glm.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/glm/test_modeling_glm.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/glm/test_modeling_glm.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/glm/test_modeling_glm.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/roformer/test_modeling_roformer.py:            input_ids[:1], num_beams=2, max_length=15, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/roformer/test_modeling_roformer.py:        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=15, do_sample=True)\n../../clones/transformers/tests/models/mllama/test_modeling_mllama.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n../../clones/transformers/tests/models/mllama/test_modeling_mllama.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n../../clones/transformers/tests/models/mllama/test_modeling_mllama.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n../../clones/transformers/tests/models/mllama/test_modeling_mllama.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n../../clones/transformers/tests/models/opt/test_modeling_opt.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/qwen2/test_modeling_qwen2.py:            input_ids, max_new_tokens=20, do_sample=True, temperature=0.3, assistant_model=assistant_model\n../../clones/transformers/tests/models/falcon/test_modeling_falcon.py:        output_ids = model.generate(**inputs, do_sample=False, max_new_tokens=19)\n../../clones/transformers/tests/models/falcon/test_modeling_falcon.py:        output_ids = model.generate(**inputs, do_sample=False, max_new_tokens=9)\n../../clones/transformers/tests/models/falcon/test_modeling_falcon.py:            model.generate(**inputs, do_sample=False, max_new_tokens=4)\n../../clones/transformers/tests/models/falcon/test_modeling_falcon.py:            model.generate(**inputs, do_sample=True, max_new_tokens=4)\n../../clones/transformers/tests/models/falcon/test_modeling_falcon.py:                outputs_no_cache = model.generate(**inputs, do_sample=False, max_new_tokens=20, use_cache=False)\n../../clones/transformers/tests/models/falcon/test_modeling_falcon.py:                outputs_cache = model.generate(**inputs, do_sample=False, max_new_tokens=20, use_cache=True)\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:                    do_sample=False,\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:                    depth_decoder_do_sample=False,\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:                    do_sample=False,\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:                    depth_decoder_do_sample=False,\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:                do_sample=False, max_new_tokens=self.max_new_tokens, remove_invalid_values=True\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:            model.generate(input_ids, max_new_tokens=5, **input_dict, depth_decoder_do_sample=True)\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:                input_ids, max_new_tokens=5, **input_dict, depth_decoder_do_sample=True, depth_decoder_num_beams=5\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:            do_sample=False,\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:            depth_decoder_do_sample=False,\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:            do_sample=False, depth_decoder_do_sample=False, return_audio_codes=True, max_new_tokens=10\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:            do_sample=False, depth_decoder_do_sample=False, return_audio_codes=True, max_new_tokens=10\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:            do_sample=False, depth_decoder_do_sample=False, return_audio_codes=True, max_new_tokens=10\n../../clones/transformers/tests/models/moshi/test_modeling_moshi.py:            do_sample=False, depth_decoder_do_sample=False, return_audio_codes=True, max_new_tokens=10\n../../clones/transformers/tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py:        model.generate(**input_dict, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/falcon_mamba/test_modeling_falcon_mamba.py:        out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/falcon_mamba/test_modeling_falcon_mamba.py:        out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/falcon_mamba/test_modeling_falcon_mamba.py:        out = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/codegen/test_modeling_codegen.py:            output_ids = model.generate(**inputs, do_sample=False)\n../../clones/transformers/tests/models/codegen/test_modeling_codegen.py:        output_ids = model.generate(input_ids, do_sample=True)\n../../clones/transformers/tests/models/codegen/test_modeling_codegen.py:        output_seq = model.generate(input_ids=input_ids, do_sample=True, num_return_sequences=5)\n../../clones/transformers/tests/models/codegen/test_modeling_codegen.py:            input_ids=input_ids, token_type_ids=token_type_ids, do_sample=True, num_return_sequences=5\n../../clones/transformers/tests/models/phi3/test_modeling_phi3.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/phi3/test_modeling_phi3.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            output_ids = model.generate(input_ids, do_sample=True, seed=[42, 0])\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            output_ids_xla = xla_generate(input_ids, do_sample=True, seed=[42, 0])\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            do_sample=False,\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            do_sample=False,\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            do_sample=False,\n../../clones/transformers/tests/models/t5/test_modeling_tf_t5.py:            do_sample=False,\n../../clones/transformers/tests/models/t5/test_modeling_flax_t5.py:        model.config.do_sample = False\n../../clones/transformers/tests/models/t5/test_modeling_flax_t5.py:        model.config.do_sample = False\n../../clones/transformers/tests/models/t5/test_modeling_flax_t5.py:            do_sample=False,\n../../clones/transformers/tests/models/t5/test_modeling_t5.py:            input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/t5/test_modeling_t5.py:        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n../../clones/transformers/tests/models/t5/test_modeling_t5.py:        model.config.do_sample = False\n../../clones/transformers/tests/models/t5/test_modeling_t5.py:            do_sample=False,\n../../clones/transformers/tests/models/t5/test_modeling_t5.py:            do_sample=False,\n../../clones/transformers/tests/models/t5/test_modeling_t5.py:        generated_ids = model.generate(**inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False)\n../../clones/transformers/tests/models/t5/test_modeling_t5.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/t5/test_modeling_t5.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/aria/test_modeling_aria.py:        output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n../../clones/transformers/tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py:        input_speech[\"do_sample\"] = True\n../../clones/transformers/tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py:        input_text[\"do_sample\"] = True\n../../clones/transformers/tests/models/pop2piano/test_modeling_pop2piano.py:            input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/pop2piano/test_modeling_pop2piano.py:        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n../../clones/transformers/tests/models/gpt_neo/test_modeling_gpt_neo.py:            output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/gpt_neo/test_modeling_gpt_neo.py:        output_ids = model.generate(input_ids, do_sample=True)\n../../clones/transformers/tests/models/gpt_neo/test_modeling_flax_gpt_neo.py:        model.do_sample = False\n../../clones/transformers/tests/models/led/test_modeling_led.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/cohere2/test_modeling_cohere2.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/cohere2/test_modeling_cohere2.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/cohere2/test_modeling_cohere2.py:        output = pipe(self.input_text, max_new_tokens=20, do_sample=False, padding=True)\n../../clones/transformers/tests/models/cohere2/test_modeling_cohere2.py:        output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/tests/models/bamba/test_modeling_bamba.py:        out = self.model.generate(input_ids, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/models/bamba/test_modeling_bamba.py:        out = self.model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:                input_dict[\"input_ids\"], attention_mask=input_dict[\"attention_mask\"], do_sample=True, max_new_tokens=10\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:        output_values = model.generate(**unconditional_inputs, do_sample=False, max_new_tokens=5)\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:        output_values = model.generate(**unconditional_inputs, do_sample=True, max_new_tokens=10)\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:            input_ids, attention_mask=attention_mask, do_sample=False, guidance_scale=None, max_new_tokens=10\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:            input_ids, attention_mask=attention_mask, do_sample=False, guidance_scale=3, max_new_tokens=10\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:            input_ids, attention_mask=attention_mask, do_sample=True, guidance_scale=None, max_new_tokens=10\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:        output_values = model.generate(**inputs, do_sample=False, guidance_scale=None, max_new_tokens=10)\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:        output_values = model.generate(**unconditional_inputs, do_sample=False, max_new_tokens=12)\n../../clones/transformers/tests/models/musicgen/test_modeling_musicgen.py:        output_values = model.generate(**inputs, do_sample=False, guidance_scale=3.0, max_new_tokens=12)\n../../clones/transformers/tests/models/granitemoeshared/test_modeling_granitemoeshared.py:        generated_ids = model.generate(**model_inputs, max_new_tokens=64, top_p=None, temperature=1, do_sample=False)\n../../clones/transformers/tests/models/mistral/test_modeling_mistral.py:            input_ids, max_new_tokens=20, do_sample=True, temperature=0.3, assistant_model=model\n../../clones/transformers/tests/models/mistral/test_modeling_mistral.py:        generated_ids = model.generate(**inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False)\n../../clones/transformers/tests/models/mistral/test_modeling_mistral.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/mistral/test_modeling_mistral.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"sliding_window\"\n../../clones/transformers/tests/models/mistral/test_modeling_mistral.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/mistral/test_modeling_mistral.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"sliding_window\"\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                    model.generate(do_sample=True, max_length=5)\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                self._check_generated_ids(model.generate(input_features, do_sample=True))\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                model.generate(input_features, do_sample=False, num_return_sequences=2)\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:            self._check_generated_ids(model.generate(input_features, do_sample=True, num_return_sequences=2))\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                input_features, do_sample=True, bad_words_ids=bad_words_ids, num_return_sequences=2\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                self._check_generated_ids(model.generate(input_features, do_sample=True, num_beams=2))\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                model.generate(input_features, do_sample=False, num_return_sequences=3, num_beams=2)\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                    do_sample=True,\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                model.generate(input_features, do_sample=False, num_beams=2, num_return_sequences=2)\n../../clones/transformers/tests/models/speech_to_text/test_modeling_tf_speech_to_text.py:                input_features, do_sample=False, bad_words_ids=bad_words_ids, num_beams=2, num_return_sequences=2\n../../clones/transformers/tests/models/speech_to_text/test_modeling_speech_to_text.py:        model.generate(input_features, num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/granitemoe/test_modeling_granitemoe.py:        generated_ids = model.generate(**model_inputs, max_new_tokens=64, top_p=None, temperature=1, do_sample=False)\n../../clones/transformers/tests/models/m2m_100/test_modeling_m2m_100.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/xlnet/test_modeling_xlnet.py:        output_ids = model.generate(input_ids, max_length=200, do_sample=False)\n../../clones/transformers/tests/models/xlnet/test_modeling_tf_xlnet.py:        output_ids = model.generate(input_ids, max_length=200, do_sample=False)\n../../clones/transformers/tests/models/llava/test_modeling_llava.py:        output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n../../clones/transformers/tests/models/instructblipvideo/test_modeling_instructblipvideo.py:        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n../../clones/transformers/tests/models/instructblipvideo/test_modeling_instructblipvideo.py:        predictions_expanded = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n../../clones/transformers/tests/models/mt5/test_modeling_mt5.py:            input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/mt5/test_modeling_mt5.py:        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n../../clones/transformers/tests/models/nllb_moe/test_modeling_nllb_moe.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=False,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=False,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=False,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=False,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=False,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=False,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=False,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=False,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:            self.model.generate(**input_ids, do_sample=True, temperature=0.6, penalty_alpha=0.6)\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:            self.model.generate(**input_ids, do_sample=True, temperature=0.6, num_beams=4)\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:        args = {\"do_sample\": False, \"temperature\": None}\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                **input_ids, do_sample=False, temperature=1.0, coarse_do_sample=True, coarse_temperature=0.7\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=True,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                coarse_do_sample=True,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:                do_sample=True,\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:            output_with_no_offload = self.model.generate(**input_ids, do_sample=False, temperature=1.0)\n../../clones/transformers/tests/models/bark/test_modeling_bark.py:            output_with_offload = self.model.generate(**input_ids, do_sample=False, temperature=1.0)\n../../clones/transformers/tests/models/zamba2/test_modeling_zamba2.py:    def test_new_cache_format(self, num_beams, do_sample):\n../../clones/transformers/tests/models/zamba2/test_modeling_zamba2.py:        out = self.model.generate(input_ids, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/models/zamba2/test_modeling_zamba2.py:        out = self.model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/models/gemma2/test_modeling_gemma2.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma2/test_modeling_gemma2.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gemma2/test_modeling_gemma2.py:        output = pipe(self.input_text, max_new_tokens=20, do_sample=False, padding=True)\n../../clones/transformers/tests/models/gemma2/test_modeling_gemma2.py:        output = pipe(self.input_text, max_new_tokens=20, do_sample=False, padding=True)\n../../clones/transformers/tests/models/gemma2/test_modeling_gemma2.py:        output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n../../clones/transformers/tests/models/gemma2/test_modeling_gemma2.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/got_ocr2/test_modeling_got_ocr2.py:            do_sample=False,\n../../clones/transformers/tests/models/got_ocr2/test_modeling_got_ocr2.py:        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n../../clones/transformers/tests/models/got_ocr2/test_modeling_got_ocr2.py:        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n../../clones/transformers/tests/models/got_ocr2/test_modeling_got_ocr2.py:        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n../../clones/transformers/tests/models/got_ocr2/test_modeling_got_ocr2.py:        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n../../clones/transformers/tests/models/got_ocr2/test_modeling_got_ocr2.py:        generate_ids = model.generate(**inputs, do_sample=False, num_beams=1, max_new_tokens=4)\n../../clones/transformers/tests/models/phimoe/test_modeling_phimoe.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/phimoe/test_modeling_phimoe.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/udop/test_modeling_udop.py:            input_ids[:1], bbox=bbox[:1, :, :], num_beams=2, max_length=5, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/udop/test_modeling_udop.py:            input_ids[:1], bbox=bbox[:1, :, :], num_beams=2, max_length=5, do_sample=True\n../../clones/transformers/tests/models/gptj/test_modeling_flax_gptj.py:        model.do_sample = False\n../../clones/transformers/tests/models/gptj/test_modeling_tf_gptj.py:        output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/gptj/test_modeling_tf_gptj.py:            output_ids = model.generate(**tokenized, do_sample=True, seed=[42, 0])\n../../clones/transformers/tests/models/gptj/test_modeling_tf_gptj.py:        outputs = model.generate(**inputs, do_sample=False, num_beams=2)\n../../clones/transformers/tests/models/gptj/test_modeling_tf_gptj.py:        output_non_padded = model.generate(**inputs_non_padded, do_sample=False, num_beams=2)\n../../clones/transformers/tests/models/gptj/test_modeling_tf_gptj.py:            **inputs_padded, do_sample=False, num_beams=2, max_length=model.config.max_length - num_paddings\n../../clones/transformers/tests/models/gptj/test_modeling_tf_gptj.py:        outputs_xla = xla_generate(**inputs, do_sample=False, num_beams=2)\n../../clones/transformers/tests/models/gptj/test_modeling_gptj.py:            output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/gptj/test_modeling_gptj.py:        output_ids = model.generate(input_ids, do_sample=True)\n../../clones/transformers/tests/models/gptj/test_modeling_gptj.py:        output_seq = model.generate(input_ids=input_ids, do_sample=True, num_return_sequences=5)\n../../clones/transformers/tests/models/gptj/test_modeling_gptj.py:            input_ids=input_ids, token_type_ids=token_type_ids, do_sample=True, num_return_sequences=5\n../../clones/transformers/tests/models/video_llava/test_modeling_video_llava.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n../../clones/transformers/tests/models/video_llava/test_modeling_video_llava.py:        output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n../../clones/transformers/tests/models/video_llava/test_modeling_video_llava.py:        output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n../../clones/transformers/tests/models/jamba/test_modeling_jamba.py:        out = self.model.generate(input_ids, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/models/jamba/test_modeling_jamba.py:        out = self.model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/models/qwen2_moe/test_modeling_qwen2_moe.py:            input_ids, max_new_tokens=20, do_sample=True, temperature=0.3, assistant_model=assistant_model\n../../clones/transformers/tests/models/emu3/test_modeling_emu3.py:        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/emu3/test_modeling_emu3.py:        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/emu3/test_modeling_emu3.py:        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/emu3/test_modeling_emu3.py:            do_sample=False,\n../../clones/transformers/tests/models/marian/test_modeling_marian.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/starcoder2/test_modeling_starcoder2.py:        output = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/starcoder2/test_modeling_starcoder2.py:        output = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/starcoder2/test_modeling_starcoder2.py:        output = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/starcoder2/test_modeling_starcoder2.py:        output = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/git/test_modeling_git.py:            do_sample=False,\n../../clones/transformers/tests/models/git/test_modeling_git.py:            do_sample=False,\n../../clones/transformers/tests/models/mvp/test_modeling_mvp.py:            do_sample=True,\n../../clones/transformers/tests/models/mvp/test_modeling_mvp.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/mvp/test_modeling_mvp.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/plbart/test_modeling_plbart.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/blenderbot/test_modeling_blenderbot.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/mamba/test_modeling_mamba.py:        out = model.generate(input_ids, do_sample=False, use_cache=True, max_new_tokens=10)\n../../clones/transformers/tests/models/xglm/test_modeling_tf_xglm.py:        output_ids = model.generate(input_ids, do_sample=False, num_beams=1)\n../../clones/transformers/tests/models/xglm/test_modeling_tf_xglm.py:            output_ids = model.generate(input_ids, do_sample=True, seed=[7, 0])\n../../clones/transformers/tests/models/xglm/test_modeling_xglm.py:        output_ids = model.generate(input_ids, do_sample=False, num_beams=1)\n../../clones/transformers/tests/models/xglm/test_modeling_xglm.py:        output_ids = model.generate(input_ids, do_sample=True, num_beams=1)\n../../clones/transformers/tests/models/xglm/test_modeling_flax_xglm.py:        model.config.do_sample = False\n../../clones/transformers/tests/models/cohere/test_modeling_cohere.py:        output = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py:        output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n../../clones/transformers/tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py:        output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n../../clones/transformers/tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py:        output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n../../clones/transformers/tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py:        output = model.generate(**inputs, max_new_tokens=128, do_sample=True)\n../../clones/transformers/tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py:        output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py:        output = model.generate(**inputs, max_new_tokens=64, do_sample=False)\n../../clones/transformers/tests/models/fsmt/test_modeling_fsmt.py:            do_sample=True,\n../../clones/transformers/tests/models/fsmt/test_modeling_fsmt.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/reformer/test_modeling_reformer.py:        output = model.generate(input_ids[:, -10:], attention_mask=input_mask, do_sample=False)\n../../clones/transformers/tests/models/reformer/test_modeling_reformer.py:            input_ids, max_length=50, num_beams=4, early_stopping=True, do_sample=False, num_hashes=8\n../../clones/transformers/tests/models/gpt2/test_modeling_gpt2.py:        output_ids = model.generate(input_ids, do_sample=False, max_length=20)\n../../clones/transformers/tests/models/gpt2/test_modeling_gpt2.py:        output_ids = model.generate(input_ids, do_sample=True, max_length=20)\n../../clones/transformers/tests/models/gpt2/test_modeling_gpt2.py:        output_seq = model.generate(input_ids=input_ids, do_sample=True, num_return_sequences=5, max_length=20)\n../../clones/transformers/tests/models/gpt2/test_modeling_gpt2.py:            input_ids=input_ids, token_type_ids=token_type_ids, do_sample=True, num_return_sequences=5, max_length=20\n../../clones/transformers/tests/models/gpt2/test_modeling_gpt2.py:        output_native = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gpt2/test_modeling_gpt2.py:        output_fa_2 = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n../../clones/transformers/tests/models/gpt2/test_modeling_flax_gpt2.py:        model.do_sample = False\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:        output_ids = model.generate(**input_ids, do_sample=False)\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:        output_ids = xla_generate(**input_ids, do_sample=False)\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:            output_ids = model.generate(**input_ids, do_sample=True, seed=[7, 0])\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:            output_ids = xla_generate(**input_ids, do_sample=True, seed=[7, 0])\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:        output_ids = model.generate(**input_ids, do_sample=False, num_beams=2)\n../../clones/transformers/tests/models/gpt2/test_modeling_tf_gpt2.py:        output_ids = xla_generate(**input_ids, do_sample=False, num_beams=2)\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:        logits_processor_kwargs = super()._get_logits_processor_kwargs(do_sample=do_sample, config=config)\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:        model.generate(input_features, num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:            input_features, do_sample=False, max_length=20, language=\"<|en|>\", task=\"transcribe\"\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:            input_features, do_sample=False, max_length=20, language=\"<|de|>\", task=\"transcribe\"\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:            input_features, do_sample=False, max_length=20, language=\"<|de|>\", task=\"translate\"\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:            do_sample=False,\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:        generated_ids = model.generate(input_features, do_sample=False, language=\"<|de|>\", task=\"transcribe\")\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:        generated_ids_forced = model.generate(input_features, do_sample=False, forced_decoder_ids=forced_decoder_ids)\n../../clones/transformers/tests/models/whisper/test_modeling_whisper.py:            \"do_sample\": False,\n../../clones/transformers/tests/models/whisper/test_modeling_flax_whisper.py:        generated_ids = model.generate(input_features, do_sample=False, max_length=20).sequences\n../../clones/transformers/tests/models/whisper/test_modeling_flax_whisper.py:            do_sample=False,\n../../clones/transformers/tests/models/whisper/test_modeling_flax_whisper.py:        generated_ids = model.generate(input_features, do_sample=False, max_length=20).sequences\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:        model.generate(input_features, num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                    model.generate(do_sample=True, max_length=5)\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                self._check_generated_ids(model.generate(input_features, do_sample=True))\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                model.generate(input_features, do_sample=False, num_return_sequences=2)\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:            self._check_generated_ids(model.generate(input_features, do_sample=True, num_return_sequences=2))\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                input_features, do_sample=True, bad_words_ids=bad_words_ids, num_return_sequences=2\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                self._check_generated_ids(model.generate(input_features, do_sample=True, num_beams=2))\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                model.generate(input_features, do_sample=False, num_return_sequences=3, num_beams=2)\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                    do_sample=True,\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                model.generate(input_features, do_sample=False, num_beams=2, num_return_sequences=2)\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:                input_features, do_sample=False, bad_words_ids=bad_words_ids, num_beams=2, num_return_sequences=2\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:            do_sample=False,\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:            do_sample=False,\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:            do_sample=False,\n../../clones/transformers/tests/models/whisper/test_modeling_tf_whisper.py:            do_sample=False,\n../../clones/transformers/tests/models/llama/test_modeling_llama.py:        generated_ids = model.generate(**model_inputs, max_new_tokens=128, do_sample=False)\n../../clones/transformers/tests/models/llama/test_modeling_llama.py:            **model_inputs, max_new_tokens=64, top_p=None, temperature=1, do_sample=False, dola_layers=\"low\"\n../../clones/transformers/tests/models/llama/test_modeling_llama.py:        generated_ids = model.generate(**inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False)\n../../clones/transformers/tests/models/llama/test_modeling_llama.py:            **inputs, max_new_tokens=NUM_TOKENS_TO_GENERATE, do_sample=False, cache_implementation=\"static\"\n../../clones/transformers/tests/models/blip_2/test_modeling_blip_2.py:        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n../../clones/transformers/tests/models/blip_2/test_modeling_blip_2.py:        predictions_expanded = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n../../clones/transformers/tests/models/zamba/test_modeling_zamba.py:        out = self.model.generate(input_ids, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/models/zamba/test_modeling_zamba.py:        out = self.model.generate(**inputs, do_sample=False, max_new_tokens=10)\n../../clones/transformers/tests/models/bart/test_modeling_bart.py:            do_sample=True,\n../../clones/transformers/tests/models/bart/test_modeling_bart.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/bart/test_modeling_bart.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/bart/test_modeling_bart.py:        expected_params = {\"num_beams\": 6, \"do_sample\": False, \"early_stopping\": True, \"length_penalty\": 1.0}\n../../clones/transformers/tests/models/paligemma/test_modeling_paligemma.py:        output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n../../clones/transformers/tests/models/paligemma/test_modeling_paligemma.py:        output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n../../clones/transformers/tests/models/nemotron/test_modeling_nemotron.py:        output = model.generate(**inputs, do_sample=False)\n../../clones/transformers/tests/models/nemotron/test_modeling_nemotron.py:        output = model.generate(**inputs, do_sample=False)\n../../clones/transformers/tests/models/nemotron/test_modeling_nemotron.py:        output = model.generate(**inputs, do_sample=False)\n../../clones/transformers/tests/models/instructblip/test_modeling_instructblip.py:            do_sample=False,\n../../clones/transformers/tests/models/instructblip/test_modeling_instructblip.py:        predictions = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n../../clones/transformers/tests/models/instructblip/test_modeling_instructblip.py:        predictions_expanded = model.generate(**inputs, do_sample=False, max_new_tokens=15)\n../../clones/transformers/tests/models/idefics/test_modeling_idefics.py:            outputs = model.generate(**inputs, do_sample=False, max_new_tokens=4, return_dict_in_generate=True)\n../../clones/transformers/tests/models/idefics/test_modeling_idefics.py:            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=3, return_dict_in_generate=True)\n../../clones/transformers/tests/models/idefics/test_modeling_idefics.py:            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=1, return_dict_in_generate=True)\n../../clones/transformers/tests/models/idefics/test_modeling_idefics.py:                do_sample=False,\n../../clones/transformers/tests/models/idefics/test_modeling_idefics.py:                \"do_sample\": False,\n../../clones/transformers/tests/models/bloom/test_modeling_bloom.py:        greedy_output = model.generate(input_ids, attention_mask=attention_mask, max_length=50, do_sample=False)\n../../clones/transformers/tests/models/bloom/test_modeling_bloom.py:        greedy_output = model.generate(input_ids, attention_mask=attention_mask, max_length=50, do_sample=False)\n../../clones/transformers/tests/models/bloom/test_modeling_bloom.py:            input_ids_without_pad.to(torch_device), max_length=50, do_sample=False\n../../clones/transformers/tests/models/gpt_neox/test_modeling_gpt_neox.py:            output_ids = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n../../clones/transformers/tests/models/gpt_neox/test_modeling_gpt_neox.py:            output_ids = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:                input_dict[\"input_ids\"], attention_mask=input_dict[\"attention_mask\"], do_sample=True, max_new_tokens=10\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:        output_values = model.generate(**unconditional_inputs, do_sample=False, max_new_tokens=10, guidance_scale=1.0)\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:            **unconditional_inputs, do_sample=True, max_new_tokens=10, guidance_scale=1.0, temperature=1.0, top_k=250\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:            input_ids, attention_mask=attention_mask, do_sample=False, guidance_scale=None, max_new_tokens=10\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:            input_ids, attention_mask=attention_mask, do_sample=False, guidance_scale=3, max_new_tokens=10\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:            do_sample=True,\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:        output_values = model.generate(**inputs, do_sample=False, guidance_scale=None, max_new_tokens=10)\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:        output_values = model.generate(**unconditional_inputs, do_sample=False, max_new_tokens=12, guidance_scale=1.0)\n../../clones/transformers/tests/models/musicgen_melody/test_modeling_musicgen_melody.py:        output_values = model.generate(**inputs, do_sample=False, guidance_scale=3.0, max_new_tokens=12)\n../../clones/transformers/tests/models/pegasus_x/test_modeling_pegasus_x.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/seamless_m4t/test_modeling_seamless_m4t.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/seamless_m4t/test_modeling_seamless_m4t.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/seamless_m4t/test_modeling_seamless_m4t.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/seamless_m4t/test_modeling_seamless_m4t.py:            \"do_sample\": True,\n../../clones/transformers/tests/models/seamless_m4t/test_modeling_seamless_m4t.py:        input_speech[\"do_sample\"] = True\n../../clones/transformers/tests/models/seamless_m4t/test_modeling_seamless_m4t.py:        input_text[\"do_sample\"] = True\n../../clones/transformers/tests/models/olmo/test_modeling_olmo.py:        generated_ids = model.generate(input_ids, max_new_tokens=64, top_p=None, temperature=1, do_sample=False)\n../../clones/transformers/tests/models/olmo/test_modeling_olmo.py:            **prompt_tokens, max_new_tokens=max_new_tokens, do_sample=False, cache_implementation=cache_implementation\n../../clones/transformers/tests/models/ctrl/test_modeling_ctrl.py:        output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/ctrl/test_modeling_tf_ctrl.py:        output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/chameleon/test_modeling_chameleon.py:        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/chameleon/test_modeling_chameleon.py:        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/chameleon/test_modeling_chameleon.py:        generated_ids = model.generate(**inputs, max_new_tokens=40, do_sample=False)\n../../clones/transformers/tests/models/mbart/test_modeling_mbart.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/longt5/test_modeling_flax_longt5.py:            do_sample=False,\n../../clones/transformers/tests/models/longt5/test_modeling_longt5.py:            input_ids[:1], num_beams=2, max_length=5, do_sample=True, use_cache=False\n../../clones/transformers/tests/models/longt5/test_modeling_longt5.py:        output_with_past_cache = model.generate(input_ids[:1], num_beams=2, max_length=5, do_sample=True)\n../../clones/transformers/tests/models/longt5/test_modeling_longt5.py:            do_sample=False,\n../../clones/transformers/tests/models/openai/test_modeling_tf_openai.py:        output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/openai/test_modeling_openai.py:        output_ids = model.generate(input_ids, do_sample=False)\n../../clones/transformers/tests/models/olmo2/test_modeling_olmo2.py:        generated_ids = model.generate(input_ids, max_new_tokens=64, top_p=None, temperature=1, do_sample=False)\n../../clones/transformers/tests/models/olmo2/test_modeling_olmo2.py:            **prompt_tokens, max_new_tokens=max_new_tokens, do_sample=False, cache_implementation=cache_implementation\n../../clones/transformers/tests/models/pegasus/test_modeling_pegasus.py:        model.generate(num_beams=4, do_sample=True, early_stopping=False, num_return_sequences=3)\n../../clones/transformers/tests/models/olmoe/test_modeling_olmoe.py:        generated_ids = model.generate(input_ids, max_new_tokens=64, top_p=None, temperature=1, do_sample=False)\n\n```\n","ts":1.739556617037E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556617693E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556618372E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556623894E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.73955662469E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.73955662979E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556630472E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556631038E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556636377E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556637047E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556638374E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.73955663894E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556640008E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556640572E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556645935E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556646822E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556647496E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556648133E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556648872E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556649432E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556650106E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556650889E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556651562E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556652124E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556657199E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556662245E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556662893E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556668091E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556668705E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556673766E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556674201E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556674584E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556679394E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556679972E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556684993E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556685659E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556690492E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739556695527E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556696151E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.739556696706E9}
{"role":"assistant","content":"","finish_reason":"{\"text\":\"\",\"logprobs\":{\"token_logprobs\":[],\"tokens\":[]},\"token_ids\":[]}","ts":1.73955669754E9}
{"role":"assistant","content":"","finish_reason":"runtime_error SIGINT","ts":1.739556698323E9}

{"role":"user","content":"vLLM's readme is at [1] https://raw.githubusercontent.com/vllm-project/vllm/main/README.md\n\n1: **https://raw.githubusercontent.com/vllm-project/vllm/main/README.md**\n```\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png\">\n    <img alt=\"vLLM\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nEasy, fast, and cheap LLM serving for everyone\n</h3>\n\n<p align=\"center\">\n| <a href=\"https://docs.vllm.ai\"><b>Documentation</b></a> | <a href=\"https://vllm.ai\"><b>Blog</b></a> | <a href=\"https://arxiv.org/abs/2309.06180\"><b>Paper</b></a> | <a href=\"https://x.com/vllm_project\"><b>Twitter/X</b></a> | <a href=\"https://slack.vllm.ai\"><b>Developer Slack</b></a> |\n</p>\n\n---\n\nWe are excited to invite you to our Menlo Park meetup with Meta, evening of Thursday, February 27! Meta engineers will discuss the improvements on top of vLLM, and vLLM contributors will share updates from the v0.7.x series of releases. [Register Now](https://lu.ma/h7g3kuj9)\n\n---\n\n*Latest News* ðŸ”¥\n\n- [2025/01] We are excited to announce the alpha release of vLLM V1: A major architectural upgrade with 1.7x speedup! Clean code, optimized execution loop, zero-overhead prefix caching, enhanced multimodal support, and more. Please check out our blog post [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).\n- [2025/01] We hosted [the eighth vLLM meetup](https://lu.ma/zep56hui) with Google Cloud! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1epVkt4Zu8Jz_S5OhEHPc798emsYh2BwYfRuDDVEF7u4/edit?usp=sharing), and Google Cloud team [here](https://drive.google.com/file/d/1h24pHewANyRL11xy5dXUbvRC9F9Kkjix/view?usp=sharing).\n- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!\n- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).\n- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!\n- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).\n- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).\n- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).\n- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).\n- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).\n- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).\n- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).\n\n---\n\n## About\n\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nOriginally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.\n\nvLLM is fast with:\n\n- State-of-the-art serving throughput\n- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)\n- Continuous batching of incoming requests\n- Fast model execution with CUDA/HIP graph\n- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), INT4, INT8, and FP8.\n- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.\n- Speculative decoding\n- Chunked prefill\n\n**Performance benchmark**: We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](.buildkite/nightly-benchmarks/) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.\n\nvLLM is flexible and easy to use with:\n\n- Seamless integration with popular Hugging Face models\n- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- Tensor parallelism and pipeline parallelism support for distributed inference\n- Streaming outputs\n- OpenAI-compatible API server\n- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.\n- Prefix caching support\n- Multi-lora support\n\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\n- Transformer-like LLMs (e.g., Llama)\n- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\n- Embedding Models (e.g. E5-Mistral)\n- Multi-modal LLMs (e.g., LLaVA)\n\nFind the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).\n\n## Getting Started\n\nInstall vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):\n\n```bash\npip install vllm\n```\n\nVisit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.\n- [Installation](https://docs.vllm.ai/en/latest/getting_started/installation/index.html)\n- [Quickstart](https://docs.vllm.ai/en/latest/getting_started/quickstart.html)\n- [List of Supported Models](https://docs.vllm.ai/en/latest/models/supported_models.html)\n\n## Contributing\n\nWe welcome and value any contributions and collaborations.\nPlease check out [CONTRIBUTING.md](./CONTRIBUTING.md) for how to get involved.\n\n## Sponsors\n\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\n\n<!-- Note: Please sort them in alphabetical order. -->\n<!-- Note: Please keep these consistent with docs/source/community/sponsors.md -->\nCash Donations:\n- a16z\n- Dropbox\n- Sequoia Capital\n- Skywork AI\n- ZhenFund\n\nCompute Resources:\n- AMD\n- Anyscale\n- AWS\n- Crusoe Cloud\n- Databricks\n- DeepInfra\n- Google Cloud\n- Lambda Lab\n- Nebius\n- Novita AI\n- NVIDIA\n- Replicate\n- Roblox\n- RunPod\n- Trainy\n- UC Berkeley\n- UC San Diego\n\nSlack Sponsor: Anyscale\n\nWe also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.\n\n## Citation\n\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n\n```bibtex\n@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n  year={2023}\n}\n```\n\n## Contact Us\n\n- For technical questions and feature requests, please use Github issues or discussions.\n- For discussing with fellow users and coordinating contributions and development, please use Slack.\n- For security disclosures, please use Github's security advisory feature.\n- For collaborations and partnerships, please contact us at vllm-questions AT lists.berkeley.edu.\n\n## Media Kit\n\n- If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).\n```\n","ts":1.739554147422E9}
{"role":"assistant","content":"The vLLM README provides a comprehensive overview of the project, its features, and its community. Hereâ€™s a summary of the key points:\n\n---\n\n### **Overview**\nvLLM is a **fast and easy-to-use library for LLM (Large Language Model) inference and serving**. It was originally developed at UC Berkeley's **Sky Computing Lab** and has grown into a **community-driven project** with contributions from academia and industry.\n\n---\n\n### **Key Features**\n- **High Performance**:\n  - State-of-the-art serving throughput.\n  - Efficient memory management with **PagedAttention**.\n  - Continuous batching of requests.\n  - Quantization support (GPTQ, AWQ, INT4, INT8, FP8).\n  - Optimized CUDA kernels (e.g., FlashAttention, FlashInfer).\n  - Speculative decoding and chunked prefill.\n\n- **Flexibility**:\n  - Seamless integration with Hugging Face models.\n  - Support for decoding algorithms like parallel sampling and beam search.\n  - Tensor and pipeline parallelism for distributed inference.\n  - OpenAI-compatible API server.\n  - Multi-LoRA support and prefix caching.\n\n- **Hardware Support**:\n  - NVIDIA GPUs, AMD CPUs/GPUs, Intel CPUs/GPUs, PowerPC CPUs, TPUs, and AWS Neuron.\n\n---\n\n### **Supported Models**\nvLLM supports a wide range of models, including:\n- Transformer-like LLMs (e.g., Llama).\n- Mixture-of-Experts LLMs (e.g., Mixtral, Deepseek-V2/V3).\n- Embedding models (e.g., E5-Mistral).\n- Multi-modal LLMs (e.g., LLaVA).\n\nA full list of supported models is available in the [documentation](https://docs.vllm.ai/en/latest/models/supported_models.html).\n\n---\n\n### **Getting Started**\n1. Install vLLM via pip:\n   ```bash\n   pip install vllm\n   ```\n2. Visit the [documentation](https://docs.vllm.ai/en/latest/) for installation instructions, quickstart guides, and more.\n\n---\n\n### **Performance Benchmark**\nvLLM has been benchmarked against other LLM serving engines (e.g., TensorRT-LLM, SGLang, LMDeploy). Details and results can be found in the [blog post](https://blog.vllm.ai/2024/09/05/perf-up","finish_reason":"length","ts":1.739554181815E9}
{"role":"assistant","content":"date.html). The benchmark can be reproduced using the [nightly-benchmarks folder](.buildkite/nightly-benchmarks/).\n\n---\n\n### **Community & Events**\n- **Meetups**: vLLM regularly hosts meetups with industry partners like Meta, Google Cloud, Snowflake, NVIDIA, AWS, and others. Slides and recordings are available online.\n- **Developer Slack**: A [Slack workspace](https://slack.vllm.ai) is available for discussions and contributions.\n- **Funding**: The project is supported by organizations like **a16z**, **Dropbox**, **Sequoia Capital**, and **Google Cloud**, among others.\n\n---\n\n### **Latest News**\n- **[2025/01]**: Alpha release of vLLM V1 with a **1.7x speedup**, optimized execution loop, and enhanced multimodal support. Learn more [here](https://blog.vllm.ai/2025/01/27/v1-alpha-release.html).\n- **[2025/01]**: Hosted the eighth vLLM meetup with Google Cloud.\n- **[2024/12]**: vLLM joined the **PyTorch ecosystem**.\n- **[2024/10]**: Launched the developer Slack workspace.\n- **[2024/07]**: Officially supports Llama 3.1 with FP8 quantization and pipeline parallelism.\n\n---\n\n### **Contributing**\nvLLM welcomes contributions! Check out [CONTRIBUTING.md](./CONTRIBUTING.md) to get involved.\n\n---\n\n### **Sponsors**\nvLLM is supported by **cash donations** and **compute resources** from organizations like **a16z**, **Dropbox**, **Google Cloud**, **NVIDIA**, **AWS**, and **UC Berkeley**. Donations can also be made via [OpenCollective](https://opencollective.com/vllm).\n\n---\n\n### **Citation**\nIf you use vLLM in your research, cite the [paper](https://arxiv.org/abs/2309.06180):\n```bibtex\n@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating","finish_reason":"length","ts":1.739554204112E9}
{"role":"assistant","content":" Systems Principles},\n  year={2023}\n}\n```\n\n---\n\n### **Contact**\n- **Technical Questions**: Use GitHub issues or discussions.\n- **Community Discussions**: Join the [Slack workspace](https://slack.vllm.ai).\n- **Collaborations**: Email `vllm-questions@lists.berkeley.edu`.\n\n---\n\n### **Media Kit**\nThe vLLM logo and branding assets are available in the [media kit repo](https://github.com/vllm-project/media-kit).\n\n---\n\nThis README highlights vLLMâ€™s focus on **speed, flexibility, and community collaboration**, making it a powerful tool for LLM inference and serving.","finish_reason":"","ts":1.739554224429E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554226198E9}
{"role":"assistant","content":"","finish_reason":"stop","ts":1.739554226961E9}
{"role":"user","content":"Is there a URL where I could fetch a listing of files in the root of their repository? Give a 2-line reply.\n","ts":1.739554253503E9}
{"role":"assistant","content":"You can fetch a listing of files in the root of the vLLM repository using the GitHub API. Try this URL: `https://api.github.com/repos/vllm-project/vllm/contents/`.","finish_reason":"","ts":1.739554259355E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554259999E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554260845E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554261489E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554262351E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554263109E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554263749E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554264514E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554265249E9}
{"role":"assistant","content":"","finish_reason":"","ts":1.739554265899E9}
{"role":"assistant","content":"","finish_reason":"stop","ts":1.739554267196E9}
